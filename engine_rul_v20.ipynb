{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed,SimpleRNN,Dropout,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.20 (main, Oct  3 2024, 07:38:01) [MSC v.1929 64 bit (AMD64)]\n",
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)\n",
    " # 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataReader():\n",
    "    df = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_train.xlsx\")\n",
    "    df_truth = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_truth.xlsx\")\n",
    "    df_merged = pd.merge(df, df_truth, on='id')\n",
    "    # Step 1: Get the maximum cycle for each engine\n",
    "    max_cycle_per_engine = df.groupby('id')['cycle'].max().reset_index()\n",
    "    max_cycle_per_engine.columns = ['id', 'max_cycle']\n",
    "\n",
    "    # Step 2: Merge the maximum cycle with the df_truth to get the actual failure cycle\n",
    "    df_merged = pd.merge(max_cycle_per_engine, df_truth, on='id')\n",
    "\n",
    "    # Step 3: Calculate the actual failure cycle (when engine will fail)\n",
    "    df_merged['failure_cycle'] = df_merged['max_cycle'] + df_merged['more']\n",
    "\n",
    "    # Step 4: Merge this back with the main DataFrame to compute remaining cycles\n",
    "    df = pd.merge(df, df_merged[['id', 'failure_cycle']], on='id')\n",
    "\n",
    "    # Step 5: Calculate remaining cycles for each row by subtracting the current cycle from the failure cycle\n",
    "    df['remaining_cycles'] = df['failure_cycle'] - df['cycle']\n",
    "    df = df.drop('failure_cycle',axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_train.xlsx\")\n",
    "df_truth = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_truth.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>more</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20426</th>\n",
       "      <td>99</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.72</td>\n",
       "      <td>1600.39</td>\n",
       "      <td>1428.03</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.29</td>\n",
       "      <td>8123.55</td>\n",
       "      <td>8.4885</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.50</td>\n",
       "      <td>23.0425</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20427</th>\n",
       "      <td>99</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.52</td>\n",
       "      <td>1605.33</td>\n",
       "      <td>1430.32</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.27</td>\n",
       "      <td>8130.99</td>\n",
       "      <td>8.5124</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.39</td>\n",
       "      <td>22.9674</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20428</th>\n",
       "      <td>99</td>\n",
       "      <td>183</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.96</td>\n",
       "      <td>1606.95</td>\n",
       "      <td>1427.90</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.31</td>\n",
       "      <td>8126.90</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.57</td>\n",
       "      <td>23.1440</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20429</th>\n",
       "      <td>99</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>644.10</td>\n",
       "      <td>1600.20</td>\n",
       "      <td>1436.54</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.33</td>\n",
       "      <td>8125.66</td>\n",
       "      <td>8.5592</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.45</td>\n",
       "      <td>23.0478</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20430</th>\n",
       "      <td>99</td>\n",
       "      <td>185</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.93</td>\n",
       "      <td>1598.42</td>\n",
       "      <td>1421.56</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8127.53</td>\n",
       "      <td>8.5425</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.1931</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20431 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0       1      1   -0.0007   -0.0004       100  518.67  641.82  1589.70   \n",
       "1       1      2    0.0019   -0.0003       100  518.67  642.15  1591.82   \n",
       "2       1      3   -0.0043    0.0003       100  518.67  642.35  1587.99   \n",
       "3       1      4    0.0007    0.0000       100  518.67  642.35  1582.79   \n",
       "4       1      5   -0.0019   -0.0002       100  518.67  642.37  1582.85   \n",
       "...    ..    ...       ...       ...       ...     ...     ...      ...   \n",
       "20426  99    181   -0.0015   -0.0001       100  518.67  643.72  1600.39   \n",
       "20427  99    182   -0.0027   -0.0003       100  518.67  643.52  1605.33   \n",
       "20428  99    183   -0.0031   -0.0003       100  518.67  643.96  1606.95   \n",
       "20429  99    184   -0.0010   -0.0001       100  518.67  644.10  1600.20   \n",
       "20430  99    185   -0.0019   -0.0004       100  518.67  643.93  1598.42   \n",
       "\n",
       "            s4     s5  ...      s13      s14     s15   s16  s17   s18  s19  \\\n",
       "0      1400.60  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100   \n",
       "1      1403.14  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100   \n",
       "2      1404.20  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100   \n",
       "3      1401.87  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100   \n",
       "4      1406.22  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100   \n",
       "...        ...    ...  ...      ...      ...     ...   ...  ...   ...  ...   \n",
       "20426  1428.03  14.62  ...  2388.29  8123.55  8.4885  0.03  396  2388  100   \n",
       "20427  1430.32  14.62  ...  2388.27  8130.99  8.5124  0.03  393  2388  100   \n",
       "20428  1427.90  14.62  ...  2388.31  8126.90  8.5374  0.03  395  2388  100   \n",
       "20429  1436.54  14.62  ...  2388.33  8125.66  8.5592  0.03  395  2388  100   \n",
       "20430  1421.56  14.62  ...  2388.24  8127.53  8.5425  0.03  397  2388  100   \n",
       "\n",
       "         s20      s21  more  \n",
       "0      39.06  23.4190    98  \n",
       "1      39.00  23.4236    98  \n",
       "2      38.95  23.3442    98  \n",
       "3      38.88  23.3739    98  \n",
       "4      38.90  23.4044    98  \n",
       "...      ...      ...   ...  \n",
       "20426  38.50  23.0425    20  \n",
       "20427  38.39  22.9674    20  \n",
       "20428  38.57  23.1440    20  \n",
       "20429  38.45  23.0478    20  \n",
       "20430  38.49  23.1931    20  \n",
       "\n",
       "[20431 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.merge(df, df_truth, on='id')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the maximum cycle for each engine\n",
    "max_cycle_per_engine = df.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_per_engine.columns = ['id', 'max_cycle']\n",
    "\n",
    "# Step 2: Merge the maximum cycle with the df_truth to get the actual failure cycle\n",
    "df_merged = pd.merge(max_cycle_per_engine, df_truth, on='id')\n",
    "\n",
    "# Step 3: Calculate the actual failure cycle (when engine will fail)\n",
    "df_merged['failure_cycle'] = df_merged['max_cycle'] + df_merged['more']\n",
    "\n",
    "# Step 4: Merge this back with the main DataFrame to compute remaining cycles\n",
    "df = pd.merge(df, df_merged[['id', 'failure_cycle']], on='id')\n",
    "\n",
    "# Step 5: Calculate remaining cycles for each row by subtracting the current cycle from the failure cycle\n",
    "df['remaining_cycles'] = df['failure_cycle'] - df['cycle']\n",
    "df = df.drop('failure_cycle',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>remaining_cycles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004       100  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003       100  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003       100  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000       100  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002       100  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5  ...      s13      s14     s15   s16  s17   s18  s19    s20      s21  \\\n",
       "0  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100  39.06  23.4190   \n",
       "1  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100  39.00  23.4236   \n",
       "2  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100  38.95  23.3442   \n",
       "3  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100  38.88  23.3739   \n",
       "4  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100  38.90  23.4044   \n",
       "\n",
       "   remaining_cycles  \n",
       "0               289  \n",
       "1               288  \n",
       "2               287  \n",
       "3               286  \n",
       "4               285  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20431 entries, 0 to 20430\n",
      "Data columns (total 27 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                20431 non-null  int64  \n",
      " 1   cycle             20431 non-null  int64  \n",
      " 2   setting1          20431 non-null  float64\n",
      " 3   setting2          20431 non-null  float64\n",
      " 4   setting3          20431 non-null  int64  \n",
      " 5   s1                20431 non-null  float64\n",
      " 6   s2                20431 non-null  float64\n",
      " 7   s3                20431 non-null  float64\n",
      " 8   s4                20431 non-null  float64\n",
      " 9   s5                20431 non-null  float64\n",
      " 10  s6                20431 non-null  float64\n",
      " 11  s7                20431 non-null  float64\n",
      " 12  s8                20431 non-null  float64\n",
      " 13  s9                20431 non-null  float64\n",
      " 14  s10               20431 non-null  float64\n",
      " 15  s11               20431 non-null  float64\n",
      " 16  s12               20431 non-null  float64\n",
      " 17  s13               20431 non-null  float64\n",
      " 18  s14               20431 non-null  float64\n",
      " 19  s15               20431 non-null  float64\n",
      " 20  s16               20431 non-null  float64\n",
      " 21  s17               20431 non-null  int64  \n",
      " 22  s18               20431 non-null  int64  \n",
      " 23  s19               20431 non-null  int64  \n",
      " 24  s20               20431 non-null  float64\n",
      " 25  s21               20431 non-null  float64\n",
      " 26  remaining_cycles  20431 non-null  int64  \n",
      "dtypes: float64(20), int64(7)\n",
      "memory usage: 4.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_3(df, window_size=30, test_size=0.1, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Generates scaled sequences and splits into training, validation, and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with engine data.\n",
    "    window_size (int): Number of time steps in each sequence.\n",
    "    test_size (float): Fraction of data to reserve for testing.\n",
    "    val_size (float): Fraction of data to reserve for validation.\n",
    "\n",
    "    Returns:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, feature_scaler, target_scaler (np.ndarray): \n",
    "    Arrays of train/val/test sequences and targets, feature and target scalers.\n",
    "    \"\"\"\n",
    "    features = [col for col in df.columns if col not in ['id', 'cycle', 'remaining_cycles']]\n",
    "    target_column = 'remaining_cycles'\n",
    "    \n",
    "    # Initialize scalers\n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Lists to collect sequences and targets\n",
    "    X_sequences = []\n",
    "    y_targets = []\n",
    "    \n",
    "    # Group by engine ID and create sliding windows before splitting\n",
    "    for engine_id, engine_data in df.groupby('id'):\n",
    "        engine_data = engine_data.sort_values(by='cycle')  # Sort by cycle to keep temporal order\n",
    "        \n",
    "        # Create sliding windows\n",
    "        for i in range(len(engine_data) - window_size):\n",
    "            X_sequence = engine_data[features].iloc[i:i + window_size].values\n",
    "            y_target = engine_data[target_column].iloc[i + window_size - 1]  # Target is RUL of last cycle in window\n",
    "            \n",
    "            X_sequences.append(X_sequence)\n",
    "            y_targets.append(y_target)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_targets = np.array(y_targets)\n",
    "    \n",
    "    # Split data into training+validation and testing\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_sequences, y_targets, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Further split the training+validation data into training and validation\n",
    "    val_ratio_adjusted = val_size / (1 - test_size)  # Adjust validation size proportionally\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_ratio_adjusted, random_state=42)\n",
    "    \n",
    "    # Fit the scalers on the training data\n",
    "    feature_scaler.fit(X_train.reshape(-1, X_train.shape[2]))  # Reshape for fitting scaler\n",
    "    target_scaler.fit(y_train.reshape(-1, 1))  # Reshape target for fitting scaler\n",
    "    \n",
    "    # Apply the scalers to the data\n",
    "    X_train = feature_scaler.transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)  # Reshape back after scaling\n",
    "    X_val = feature_scaler.transform(X_val.reshape(-1, X_val.shape[2])).reshape(X_val.shape)\n",
    "    X_test = feature_scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "    \n",
    "    y_train = target_scaler.transform(y_train.reshape(-1, 1))\n",
    "    y_val = target_scaler.transform(y_val.reshape(-1, 1))\n",
    "    y_test = target_scaler.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, feature_scaler, target_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, title,lim=(None,None)):\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.ylim(lim)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted evaluation function\n",
    "def evaluate_model(model, X_test, y_test,type,scaler):\n",
    "    predictions = model.predict(X_test)\n",
    "    if predictions.shape[0] != y_test.shape[0]:\n",
    "        # Trimming to the minimum size for consistency\n",
    "        min_len = min(predictions.shape[0], y_test.shape[0])\n",
    "        predictions = predictions[:min_len]\n",
    "        y_test = y_test[:min_len]\n",
    "    rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test.reshape(-1,1)), scaler.inverse_transform(predictions.reshape(-1,1))))\n",
    "    mae = mean_absolute_error(scaler.inverse_transform(y_test.reshape(-1,1)), scaler.inverse_transform(predictions.reshape(-1,1)))\n",
    "    print(f\"{type} Model Evaluation - RMSE: {rmse:.2f}, MAE: {mae:.2f}\")\n",
    "    return predictions, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results = []\n",
    "window_size = [10,15,20,25,30,35,40,45,50]\n",
    "models = []\n",
    "historys = []\n",
    "num_features=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "851/851 [==============================] - 44s 30ms/step - loss: 0.6145 - mse: 0.5933 - val_loss: 0.5983 - val_mse: 0.5683\n",
      "Epoch 2/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.5973 - mse: 0.5649 - val_loss: 0.5871 - val_mse: 0.5514\n",
      "Epoch 3/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.5863 - mse: 0.5490 - val_loss: 0.5856 - val_mse: 0.5588\n",
      "Epoch 4/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.5720 - mse: 0.5315 - val_loss: 0.5635 - val_mse: 0.5364\n",
      "Epoch 5/200\n",
      "851/851 [==============================] - 28s 32ms/step - loss: 0.5582 - mse: 0.5141 - val_loss: 0.5570 - val_mse: 0.5106\n",
      "Epoch 6/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.5358 - mse: 0.4849 - val_loss: 0.5267 - val_mse: 0.4784\n",
      "Epoch 7/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.5108 - mse: 0.4473 - val_loss: 0.5183 - val_mse: 0.4768\n",
      "Epoch 8/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.4849 - mse: 0.4139 - val_loss: 0.4902 - val_mse: 0.4201\n",
      "Epoch 9/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.4582 - mse: 0.3760 - val_loss: 0.4603 - val_mse: 0.3832\n",
      "Epoch 10/200\n",
      "851/851 [==============================] - 27s 31ms/step - loss: 0.4369 - mse: 0.3447 - val_loss: 0.4664 - val_mse: 0.3857\n",
      "Epoch 11/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.4114 - mse: 0.3109 - val_loss: 0.4285 - val_mse: 0.3260\n",
      "Epoch 12/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.3922 - mse: 0.2844 - val_loss: 0.4091 - val_mse: 0.3055\n",
      "Epoch 13/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.3754 - mse: 0.2615 - val_loss: 0.4049 - val_mse: 0.2938\n",
      "Epoch 14/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.3564 - mse: 0.2362 - val_loss: 0.3777 - val_mse: 0.2678\n",
      "Epoch 15/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.3418 - mse: 0.2158 - val_loss: 0.3665 - val_mse: 0.2493\n",
      "Epoch 16/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.3268 - mse: 0.1996 - val_loss: 0.3446 - val_mse: 0.2235\n",
      "Epoch 17/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.3148 - mse: 0.1839 - val_loss: 0.3625 - val_mse: 0.2402\n",
      "Epoch 18/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.3046 - mse: 0.1717 - val_loss: 0.3231 - val_mse: 0.2014\n",
      "Epoch 19/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.2949 - mse: 0.1609 - val_loss: 0.3175 - val_mse: 0.1943\n",
      "Epoch 20/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.2824 - mse: 0.1470 - val_loss: 0.3072 - val_mse: 0.1874\n",
      "Epoch 21/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.2743 - mse: 0.1400 - val_loss: 0.3066 - val_mse: 0.1821\n",
      "Epoch 22/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.2607 - mse: 0.1265 - val_loss: 0.3309 - val_mse: 0.1971\n",
      "Epoch 23/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.2569 - mse: 0.1221 - val_loss: 0.2820 - val_mse: 0.1540\n",
      "Epoch 24/200\n",
      "851/851 [==============================] - 27s 31ms/step - loss: 0.2510 - mse: 0.1169 - val_loss: 0.3043 - val_mse: 0.1695\n",
      "Epoch 25/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.2452 - mse: 0.1106 - val_loss: 0.2970 - val_mse: 0.1634\n",
      "Epoch 26/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.2417 - mse: 0.1073 - val_loss: 0.2697 - val_mse: 0.1443\n",
      "Epoch 27/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.2353 - mse: 0.1008 - val_loss: 0.2684 - val_mse: 0.1431\n",
      "Epoch 28/200\n",
      "851/851 [==============================] - 27s 31ms/step - loss: 0.2301 - mse: 0.0977 - val_loss: 0.2728 - val_mse: 0.1459\n",
      "Epoch 29/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.2244 - mse: 0.0920 - val_loss: 0.2635 - val_mse: 0.1372\n",
      "Epoch 30/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.2204 - mse: 0.0894 - val_loss: 0.2657 - val_mse: 0.1373\n",
      "Epoch 31/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.2170 - mse: 0.0863 - val_loss: 0.2519 - val_mse: 0.1263\n",
      "Epoch 32/200\n",
      "851/851 [==============================] - 27s 31ms/step - loss: 0.2121 - mse: 0.0819 - val_loss: 0.2592 - val_mse: 0.1291\n",
      "Epoch 33/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.2109 - mse: 0.0813 - val_loss: 0.2459 - val_mse: 0.1238\n",
      "Epoch 34/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.2070 - mse: 0.0778 - val_loss: 0.2489 - val_mse: 0.1209\n",
      "Epoch 35/200\n",
      "851/851 [==============================] - 24s 29ms/step - loss: 0.2038 - mse: 0.0752 - val_loss: 0.2595 - val_mse: 0.1305\n",
      "Epoch 36/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.1983 - mse: 0.0719 - val_loss: 0.2411 - val_mse: 0.1163\n",
      "Epoch 37/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.1950 - mse: 0.0690 - val_loss: 0.2426 - val_mse: 0.1149\n",
      "Epoch 38/200\n",
      "851/851 [==============================] - 29s 34ms/step - loss: 0.1936 - mse: 0.0679 - val_loss: 0.2343 - val_mse: 0.1094\n",
      "Epoch 39/200\n",
      "851/851 [==============================] - 28s 33ms/step - loss: 0.1916 - mse: 0.0673 - val_loss: 0.2424 - val_mse: 0.1128\n",
      "Epoch 40/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.1916 - mse: 0.0661 - val_loss: 0.2404 - val_mse: 0.1147\n",
      "Epoch 41/200\n",
      "851/851 [==============================] - 28s 33ms/step - loss: 0.1850 - mse: 0.0624 - val_loss: 0.2309 - val_mse: 0.1081\n",
      "Epoch 42/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.1851 - mse: 0.0629 - val_loss: 0.2367 - val_mse: 0.1098\n",
      "Epoch 43/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.1807 - mse: 0.0595 - val_loss: 0.2281 - val_mse: 0.1001\n",
      "Epoch 44/200\n",
      "851/851 [==============================] - 30s 35ms/step - loss: 0.1812 - mse: 0.0594 - val_loss: 0.2319 - val_mse: 0.1062\n",
      "Epoch 45/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.1803 - mse: 0.0585 - val_loss: 0.2359 - val_mse: 0.1101\n",
      "Epoch 46/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1791 - mse: 0.0578 - val_loss: 0.2290 - val_mse: 0.1033\n",
      "Epoch 47/200\n",
      "851/851 [==============================] - 30s 35ms/step - loss: 0.1786 - mse: 0.0576 - val_loss: 0.2366 - val_mse: 0.1106\n",
      "Epoch 48/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.1751 - mse: 0.0552 - val_loss: 0.2478 - val_mse: 0.1171\n",
      "Epoch 49/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.1736 - mse: 0.0539 - val_loss: 0.2268 - val_mse: 0.1003\n",
      "Epoch 50/200\n",
      "851/851 [==============================] - 27s 32ms/step - loss: 0.1723 - mse: 0.0530 - val_loss: 0.2311 - val_mse: 0.1013\n",
      "Epoch 51/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1712 - mse: 0.0525 - val_loss: 0.2316 - val_mse: 0.1054\n",
      "Epoch 52/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1703 - mse: 0.0520 - val_loss: 0.2237 - val_mse: 0.0992\n",
      "Epoch 53/200\n",
      "851/851 [==============================] - 24s 29ms/step - loss: 0.1685 - mse: 0.0503 - val_loss: 0.2330 - val_mse: 0.1052\n",
      "Epoch 54/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1662 - mse: 0.0493 - val_loss: 0.2189 - val_mse: 0.0916\n",
      "Epoch 55/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.1682 - mse: 0.0511 - val_loss: 0.2199 - val_mse: 0.0930\n",
      "Epoch 56/200\n",
      "851/851 [==============================] - 26s 31ms/step - loss: 0.1643 - mse: 0.0480 - val_loss: 0.2286 - val_mse: 0.0964\n",
      "Epoch 57/200\n",
      "851/851 [==============================] - 27s 32ms/step - loss: 0.1634 - mse: 0.0480 - val_loss: 0.2272 - val_mse: 0.0990\n",
      "Epoch 58/200\n",
      "851/851 [==============================] - 27s 31ms/step - loss: 0.1628 - mse: 0.0468 - val_loss: 0.2167 - val_mse: 0.0933\n",
      "Epoch 59/200\n",
      "851/851 [==============================] - 27s 32ms/step - loss: 0.1607 - mse: 0.0460 - val_loss: 0.2231 - val_mse: 0.0955\n",
      "Epoch 60/200\n",
      "851/851 [==============================] - 28s 33ms/step - loss: 0.1598 - mse: 0.0455 - val_loss: 0.2187 - val_mse: 0.0950\n",
      "Epoch 61/200\n",
      "851/851 [==============================] - 27s 32ms/step - loss: 0.1598 - mse: 0.0452 - val_loss: 0.2179 - val_mse: 0.0920\n",
      "Epoch 62/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.1594 - mse: 0.0452 - val_loss: 0.2172 - val_mse: 0.0927\n",
      "Epoch 63/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1592 - mse: 0.0454 - val_loss: 0.2211 - val_mse: 0.0962\n",
      "Epoch 64/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.1558 - mse: 0.0430 - val_loss: 0.2402 - val_mse: 0.1100\n",
      "Epoch 65/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.1560 - mse: 0.0439 - val_loss: 0.2131 - val_mse: 0.0875\n",
      "Epoch 66/200\n",
      "851/851 [==============================] - 19s 22ms/step - loss: 0.1551 - mse: 0.0429 - val_loss: 0.2143 - val_mse: 0.0905\n",
      "Epoch 67/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1551 - mse: 0.0430 - val_loss: 0.2167 - val_mse: 0.0926\n",
      "Epoch 68/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1547 - mse: 0.0422 - val_loss: 0.2135 - val_mse: 0.0892\n",
      "Epoch 69/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.1530 - mse: 0.0418 - val_loss: 0.2084 - val_mse: 0.0879\n",
      "Epoch 70/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1512 - mse: 0.0404 - val_loss: 0.2167 - val_mse: 0.0923\n",
      "Epoch 71/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.1519 - mse: 0.0415 - val_loss: 0.2204 - val_mse: 0.0935\n",
      "Epoch 72/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.1514 - mse: 0.0406 - val_loss: 0.2201 - val_mse: 0.0961\n",
      "Epoch 73/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.1520 - mse: 0.0407 - val_loss: 0.2134 - val_mse: 0.0903\n",
      "Epoch 74/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.1481 - mse: 0.0389 - val_loss: 0.2148 - val_mse: 0.0914\n",
      "Epoch 75/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.1494 - mse: 0.0392 - val_loss: 0.2168 - val_mse: 0.0924\n",
      "Epoch 76/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.1474 - mse: 0.0387 - val_loss: 0.2142 - val_mse: 0.0922\n",
      "Epoch 77/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.1483 - mse: 0.0393 - val_loss: 0.2096 - val_mse: 0.0877\n",
      "Epoch 78/200\n",
      "851/851 [==============================] - 24s 29ms/step - loss: 0.1451 - mse: 0.0368 - val_loss: 0.2192 - val_mse: 0.0921\n",
      "Epoch 79/200\n",
      "851/851 [==============================] - 25s 29ms/step - loss: 0.1451 - mse: 0.0370 - val_loss: 0.2200 - val_mse: 0.0952\n",
      "Epoch 80/200\n",
      "851/851 [==============================] - 24s 29ms/step - loss: 0.1449 - mse: 0.0370 - val_loss: 0.2146 - val_mse: 0.0893\n",
      "Epoch 81/200\n",
      "851/851 [==============================] - 24s 29ms/step - loss: 0.1456 - mse: 0.0373 - val_loss: 0.2085 - val_mse: 0.0857\n",
      "Epoch 82/200\n",
      "851/851 [==============================] - 26s 30ms/step - loss: 0.1437 - mse: 0.0359 - val_loss: 0.2111 - val_mse: 0.0875\n",
      "Epoch 83/200\n",
      "851/851 [==============================] - 33s 38ms/step - loss: 0.1448 - mse: 0.0373 - val_loss: 0.2061 - val_mse: 0.0854\n",
      "Epoch 84/200\n",
      "851/851 [==============================] - 33s 39ms/step - loss: 0.1425 - mse: 0.0355 - val_loss: 0.2114 - val_mse: 0.0886\n",
      "Epoch 85/200\n",
      "851/851 [==============================] - 33s 39ms/step - loss: 0.1430 - mse: 0.0359 - val_loss: 0.2157 - val_mse: 0.0926\n",
      "Epoch 86/200\n",
      "851/851 [==============================] - 34s 40ms/step - loss: 0.1431 - mse: 0.0360 - val_loss: 0.2207 - val_mse: 0.0987\n",
      "Epoch 87/200\n",
      "851/851 [==============================] - 35s 41ms/step - loss: 0.1408 - mse: 0.0352 - val_loss: 0.2120 - val_mse: 0.0902\n",
      "Epoch 88/200\n",
      "851/851 [==============================] - 33s 39ms/step - loss: 0.1403 - mse: 0.0346 - val_loss: 0.2116 - val_mse: 0.0889\n",
      "Epoch 89/200\n",
      "851/851 [==============================] - 34s 40ms/step - loss: 0.1422 - mse: 0.0356 - val_loss: 0.2180 - val_mse: 0.0954\n",
      "Epoch 90/200\n",
      "851/851 [==============================] - 35s 41ms/step - loss: 0.1388 - mse: 0.0335 - val_loss: 0.2112 - val_mse: 0.0887\n",
      "Epoch 91/200\n",
      "851/851 [==============================] - 34s 40ms/step - loss: 0.1401 - mse: 0.0345 - val_loss: 0.2205 - val_mse: 0.0988\n",
      "Epoch 92/200\n",
      "851/851 [==============================] - 34s 40ms/step - loss: 0.1400 - mse: 0.0343 - val_loss: 0.2253 - val_mse: 0.1003\n",
      "Epoch 93/200\n",
      "851/851 [==============================] - 33s 39ms/step - loss: 0.1371 - mse: 0.0331 - val_loss: 0.2111 - val_mse: 0.0906\n",
      "Epoch 94/200\n",
      "851/851 [==============================] - 35s 41ms/step - loss: 0.1382 - mse: 0.0332 - val_loss: 0.2112 - val_mse: 0.0889\n",
      "Epoch 95/200\n",
      "851/851 [==============================] - 33s 39ms/step - loss: 0.1354 - mse: 0.0319 - val_loss: 0.2096 - val_mse: 0.0871\n",
      "Epoch 96/200\n",
      "851/851 [==============================] - 34s 40ms/step - loss: 0.1367 - mse: 0.0331 - val_loss: 0.2076 - val_mse: 0.0855\n",
      "Epoch 97/200\n",
      "851/851 [==============================] - 35s 41ms/step - loss: 0.1361 - mse: 0.0324 - val_loss: 0.2082 - val_mse: 0.0863\n",
      "Epoch 98/200\n",
      "851/851 [==============================] - 35s 41ms/step - loss: 0.1366 - mse: 0.0332 - val_loss: 0.2150 - val_mse: 0.0917\n",
      "Epoch 99/200\n",
      "851/851 [==============================] - 34s 40ms/step - loss: 0.1377 - mse: 0.0328 - val_loss: 0.2088 - val_mse: 0.0879\n",
      "Epoch 100/200\n",
      "851/851 [==============================] - 34s 40ms/step - loss: 0.1354 - mse: 0.0322 - val_loss: 0.2170 - val_mse: 0.0930\n",
      "Epoch 101/200\n",
      "851/851 [==============================] - 33s 39ms/step - loss: 0.1353 - mse: 0.0322 - val_loss: 0.2129 - val_mse: 0.0901\n",
      "Epoch 102/200\n",
      "851/851 [==============================] - 33s 39ms/step - loss: 0.1334 - mse: 0.0310 - val_loss: 0.2032 - val_mse: 0.0857\n",
      "Epoch 103/200\n",
      "851/851 [==============================] - 31s 36ms/step - loss: 0.1363 - mse: 0.0324 - val_loss: 0.2071 - val_mse: 0.0859\n",
      "Epoch 104/200\n",
      "851/851 [==============================] - 13s 16ms/step - loss: 0.1348 - mse: 0.0322 - val_loss: 0.2129 - val_mse: 0.0871\n",
      "Epoch 105/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1354 - mse: 0.0320 - val_loss: 0.2088 - val_mse: 0.0892\n",
      "Epoch 106/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1317 - mse: 0.0300 - val_loss: 0.2095 - val_mse: 0.0908\n",
      "Epoch 107/200\n",
      "851/851 [==============================] - 23s 28ms/step - loss: 0.1330 - mse: 0.0311 - val_loss: 0.2155 - val_mse: 0.0947\n",
      "Epoch 108/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.1346 - mse: 0.0322 - val_loss: 0.2051 - val_mse: 0.0871\n",
      "Epoch 109/200\n",
      "851/851 [==============================] - 16s 19ms/step - loss: 0.1338 - mse: 0.0313 - val_loss: 0.2178 - val_mse: 0.0965\n",
      "Epoch 110/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1314 - mse: 0.0307 - val_loss: 0.2144 - val_mse: 0.0938\n",
      "Epoch 111/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1309 - mse: 0.0299 - val_loss: 0.2016 - val_mse: 0.0830\n",
      "Epoch 112/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1317 - mse: 0.0305 - val_loss: 0.2158 - val_mse: 0.0939\n",
      "Epoch 113/200\n",
      "851/851 [==============================] - 12s 15ms/step - loss: 0.1304 - mse: 0.0293 - val_loss: 0.2104 - val_mse: 0.0907\n",
      "Epoch 114/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1307 - mse: 0.0300 - val_loss: 0.2147 - val_mse: 0.0932\n",
      "Epoch 115/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1295 - mse: 0.0293 - val_loss: 0.2095 - val_mse: 0.0906\n",
      "Epoch 116/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1307 - mse: 0.0300 - val_loss: 0.2183 - val_mse: 0.0955\n",
      "Epoch 117/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1302 - mse: 0.0296 - val_loss: 0.2139 - val_mse: 0.0927\n",
      "Epoch 118/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1293 - mse: 0.0292 - val_loss: 0.2113 - val_mse: 0.0926\n",
      "Epoch 119/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1304 - mse: 0.0295 - val_loss: 0.2111 - val_mse: 0.0898\n",
      "Epoch 120/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1278 - mse: 0.0285 - val_loss: 0.2051 - val_mse: 0.0887\n",
      "Epoch 121/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1287 - mse: 0.0291 - val_loss: 0.2281 - val_mse: 0.1062\n",
      "Epoch 122/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1294 - mse: 0.0299 - val_loss: 0.2081 - val_mse: 0.0883\n",
      "Epoch 123/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1282 - mse: 0.0290 - val_loss: 0.2166 - val_mse: 0.0953\n",
      "Epoch 124/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1273 - mse: 0.0287 - val_loss: 0.2103 - val_mse: 0.0892\n",
      "Epoch 125/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1269 - mse: 0.0283 - val_loss: 0.2248 - val_mse: 0.1052\n",
      "Epoch 126/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1276 - mse: 0.0282 - val_loss: 0.2126 - val_mse: 0.0930\n",
      "Epoch 127/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1265 - mse: 0.0285 - val_loss: 0.2073 - val_mse: 0.0898\n",
      "Epoch 128/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1265 - mse: 0.0284 - val_loss: 0.2106 - val_mse: 0.0908\n",
      "Epoch 129/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1262 - mse: 0.0281 - val_loss: 0.2122 - val_mse: 0.0931\n",
      "Epoch 130/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1257 - mse: 0.0277 - val_loss: 0.2115 - val_mse: 0.0919\n",
      "Epoch 131/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1262 - mse: 0.0281 - val_loss: 0.2295 - val_mse: 0.1073\n",
      "Epoch 132/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1254 - mse: 0.0276 - val_loss: 0.2121 - val_mse: 0.0917\n",
      "Epoch 133/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1249 - mse: 0.0272 - val_loss: 0.2146 - val_mse: 0.0950\n",
      "Epoch 134/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1265 - mse: 0.0281 - val_loss: 0.2154 - val_mse: 0.0948\n",
      "Epoch 135/200\n",
      "851/851 [==============================] - 11s 14ms/step - loss: 0.1270 - mse: 0.0283 - val_loss: 0.2111 - val_mse: 0.0918\n",
      "Epoch 136/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1265 - mse: 0.0282 - val_loss: 0.2103 - val_mse: 0.0932\n",
      "Epoch 137/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1237 - mse: 0.0267 - val_loss: 0.2164 - val_mse: 0.0953\n",
      "Epoch 138/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1242 - mse: 0.0272 - val_loss: 0.2139 - val_mse: 0.0958\n",
      "Epoch 139/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1218 - mse: 0.0259 - val_loss: 0.2086 - val_mse: 0.0885\n",
      "Epoch 140/200\n",
      "851/851 [==============================] - 12s 14ms/step - loss: 0.1232 - mse: 0.0267 - val_loss: 0.2215 - val_mse: 0.0994\n",
      "Epoch 141/200\n",
      "851/851 [==============================] - 11s 13ms/step - loss: 0.1232 - mse: 0.0267 - val_loss: 0.2138 - val_mse: 0.0934\n",
      "window_len 10 model Model Evaluation - RMSE: 23.04, MAE: 16.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\.conda\\envs\\mlEnv_2\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1020: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(self.var_), copy=False, constant_mask=constant_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "829/829 [==============================] - 18s 17ms/step - loss: 0.6148 - mse: 0.5921 - val_loss: 0.6061 - val_mse: 0.5661\n",
      "Epoch 2/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.5986 - mse: 0.5649 - val_loss: 0.5817 - val_mse: 0.5405\n",
      "Epoch 3/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.5842 - mse: 0.5446 - val_loss: 0.5721 - val_mse: 0.5458\n",
      "Epoch 4/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.5601 - mse: 0.5095 - val_loss: 0.5421 - val_mse: 0.4794\n",
      "Epoch 5/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.5315 - mse: 0.4726 - val_loss: 0.5061 - val_mse: 0.4335\n",
      "Epoch 6/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.4896 - mse: 0.4132 - val_loss: 0.4967 - val_mse: 0.4363\n",
      "Epoch 7/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.4504 - mse: 0.3627 - val_loss: 0.4380 - val_mse: 0.3430\n",
      "Epoch 8/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.4127 - mse: 0.3117 - val_loss: 0.4068 - val_mse: 0.3082\n",
      "Epoch 9/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.3753 - mse: 0.2602 - val_loss: 0.3599 - val_mse: 0.2421\n",
      "Epoch 10/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.3457 - mse: 0.2197 - val_loss: 0.3411 - val_mse: 0.2124\n",
      "Epoch 11/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.3148 - mse: 0.1837 - val_loss: 0.3105 - val_mse: 0.1842\n",
      "Epoch 12/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.2943 - mse: 0.1603 - val_loss: 0.2841 - val_mse: 0.1547\n",
      "Epoch 13/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.2748 - mse: 0.1389 - val_loss: 0.2728 - val_mse: 0.1449\n",
      "Epoch 14/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.2558 - mse: 0.1193 - val_loss: 0.2569 - val_mse: 0.1307\n",
      "Epoch 15/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.2398 - mse: 0.1056 - val_loss: 0.2354 - val_mse: 0.1076\n",
      "Epoch 16/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.2298 - mse: 0.0971 - val_loss: 0.2239 - val_mse: 0.0993\n",
      "Epoch 17/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.2170 - mse: 0.0855 - val_loss: 0.2145 - val_mse: 0.0863\n",
      "Epoch 18/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.2109 - mse: 0.0801 - val_loss: 0.2184 - val_mse: 0.0873\n",
      "Epoch 19/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.2016 - mse: 0.0736 - val_loss: 0.2013 - val_mse: 0.0750\n",
      "Epoch 20/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1971 - mse: 0.0709 - val_loss: 0.1888 - val_mse: 0.0669\n",
      "Epoch 21/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1912 - mse: 0.0669 - val_loss: 0.1845 - val_mse: 0.0673\n",
      "Epoch 22/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1824 - mse: 0.0598 - val_loss: 0.1824 - val_mse: 0.0601\n",
      "Epoch 23/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1809 - mse: 0.0582 - val_loss: 0.1740 - val_mse: 0.0572\n",
      "Epoch 24/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1741 - mse: 0.0537 - val_loss: 0.1698 - val_mse: 0.0545\n",
      "Epoch 25/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1696 - mse: 0.0515 - val_loss: 0.1623 - val_mse: 0.0496\n",
      "Epoch 26/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1654 - mse: 0.0482 - val_loss: 0.1574 - val_mse: 0.0449\n",
      "Epoch 27/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1617 - mse: 0.0461 - val_loss: 0.1594 - val_mse: 0.0472\n",
      "Epoch 28/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1623 - mse: 0.0466 - val_loss: 0.1698 - val_mse: 0.0513\n",
      "Epoch 29/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1579 - mse: 0.0434 - val_loss: 0.1478 - val_mse: 0.0407\n",
      "Epoch 30/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1545 - mse: 0.0419 - val_loss: 0.1616 - val_mse: 0.0459\n",
      "Epoch 31/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1515 - mse: 0.0400 - val_loss: 0.1415 - val_mse: 0.0363\n",
      "Epoch 32/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1488 - mse: 0.0387 - val_loss: 0.1471 - val_mse: 0.0408\n",
      "Epoch 33/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1462 - mse: 0.0373 - val_loss: 0.1467 - val_mse: 0.0396\n",
      "Epoch 34/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1463 - mse: 0.0373 - val_loss: 0.1473 - val_mse: 0.0391\n",
      "Epoch 35/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1451 - mse: 0.0366 - val_loss: 0.1481 - val_mse: 0.0398\n",
      "Epoch 36/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1418 - mse: 0.0351 - val_loss: 0.1651 - val_mse: 0.0478\n",
      "Epoch 37/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1411 - mse: 0.0348 - val_loss: 0.1497 - val_mse: 0.0416\n",
      "Epoch 38/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1366 - mse: 0.0326 - val_loss: 0.1446 - val_mse: 0.0383\n",
      "Epoch 39/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1366 - mse: 0.0329 - val_loss: 0.1442 - val_mse: 0.0383\n",
      "Epoch 40/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1346 - mse: 0.0314 - val_loss: 0.1380 - val_mse: 0.0335\n",
      "Epoch 41/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1333 - mse: 0.0311 - val_loss: 0.1450 - val_mse: 0.0370\n",
      "Epoch 42/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1327 - mse: 0.0303 - val_loss: 0.1432 - val_mse: 0.0365\n",
      "Epoch 43/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.1315 - mse: 0.0300 - val_loss: 0.1474 - val_mse: 0.0379\n",
      "Epoch 44/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.1303 - mse: 0.0299 - val_loss: 0.1350 - val_mse: 0.0330\n",
      "Epoch 45/200\n",
      "829/829 [==============================] - 13s 16ms/step - loss: 0.1286 - mse: 0.0288 - val_loss: 0.1307 - val_mse: 0.0319\n",
      "Epoch 46/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.1281 - mse: 0.0286 - val_loss: 0.1537 - val_mse: 0.0407\n",
      "Epoch 47/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1256 - mse: 0.0273 - val_loss: 0.1401 - val_mse: 0.0352\n",
      "Epoch 48/200\n",
      "829/829 [==============================] - 13s 16ms/step - loss: 0.1246 - mse: 0.0267 - val_loss: 0.1356 - val_mse: 0.0344\n",
      "Epoch 49/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.1259 - mse: 0.0276 - val_loss: 0.1435 - val_mse: 0.0367\n",
      "Epoch 50/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.1241 - mse: 0.0268 - val_loss: 0.1423 - val_mse: 0.0359\n",
      "Epoch 51/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1220 - mse: 0.0255 - val_loss: 0.1277 - val_mse: 0.0306\n",
      "Epoch 52/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1213 - mse: 0.0257 - val_loss: 0.1411 - val_mse: 0.0358\n",
      "Epoch 53/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1204 - mse: 0.0250 - val_loss: 0.1442 - val_mse: 0.0360\n",
      "Epoch 54/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1195 - mse: 0.0248 - val_loss: 0.1570 - val_mse: 0.0445\n",
      "Epoch 55/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1190 - mse: 0.0245 - val_loss: 0.1358 - val_mse: 0.0337\n",
      "Epoch 56/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1161 - mse: 0.0232 - val_loss: 0.1406 - val_mse: 0.0355\n",
      "Epoch 57/200\n",
      "829/829 [==============================] - 11s 14ms/step - loss: 0.1177 - mse: 0.0237 - val_loss: 0.1352 - val_mse: 0.0321\n",
      "Epoch 58/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1137 - mse: 0.0224 - val_loss: 0.1486 - val_mse: 0.0371\n",
      "Epoch 59/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1140 - mse: 0.0225 - val_loss: 0.1366 - val_mse: 0.0345\n",
      "Epoch 60/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1143 - mse: 0.0226 - val_loss: 0.1430 - val_mse: 0.0377\n",
      "Epoch 61/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1143 - mse: 0.0226 - val_loss: 0.1284 - val_mse: 0.0303\n",
      "Epoch 62/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1123 - mse: 0.0216 - val_loss: 0.1385 - val_mse: 0.0345\n",
      "Epoch 63/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1119 - mse: 0.0218 - val_loss: 0.1329 - val_mse: 0.0324\n",
      "Epoch 64/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1108 - mse: 0.0212 - val_loss: 0.1389 - val_mse: 0.0357\n",
      "Epoch 65/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1104 - mse: 0.0210 - val_loss: 0.1366 - val_mse: 0.0347\n",
      "Epoch 66/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1103 - mse: 0.0214 - val_loss: 0.1333 - val_mse: 0.0326\n",
      "Epoch 67/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1084 - mse: 0.0202 - val_loss: 0.1186 - val_mse: 0.0261\n",
      "Epoch 68/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1083 - mse: 0.0203 - val_loss: 0.1428 - val_mse: 0.0369\n",
      "Epoch 69/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1078 - mse: 0.0204 - val_loss: 0.1351 - val_mse: 0.0348\n",
      "Epoch 70/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1104 - mse: 0.0211 - val_loss: 0.1294 - val_mse: 0.0300\n",
      "Epoch 71/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1054 - mse: 0.0192 - val_loss: 0.1241 - val_mse: 0.0285\n",
      "Epoch 72/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1064 - mse: 0.0196 - val_loss: 0.1310 - val_mse: 0.0305\n",
      "Epoch 73/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1062 - mse: 0.0196 - val_loss: 0.1115 - val_mse: 0.0235\n",
      "Epoch 74/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1037 - mse: 0.0185 - val_loss: 0.1235 - val_mse: 0.0272\n",
      "Epoch 75/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1063 - mse: 0.0197 - val_loss: 0.1312 - val_mse: 0.0317\n",
      "Epoch 76/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1045 - mse: 0.0190 - val_loss: 0.1344 - val_mse: 0.0326\n",
      "Epoch 77/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1037 - mse: 0.0185 - val_loss: 0.1264 - val_mse: 0.0292\n",
      "Epoch 78/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1033 - mse: 0.0183 - val_loss: 0.1207 - val_mse: 0.0266\n",
      "Epoch 79/200\n",
      "829/829 [==============================] - 12s 14ms/step - loss: 0.1028 - mse: 0.0183 - val_loss: 0.1212 - val_mse: 0.0256\n",
      "Epoch 80/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1028 - mse: 0.0183 - val_loss: 0.1275 - val_mse: 0.0316\n",
      "Epoch 81/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1023 - mse: 0.0181 - val_loss: 0.1228 - val_mse: 0.0268\n",
      "Epoch 82/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1018 - mse: 0.0176 - val_loss: 0.1466 - val_mse: 0.0373\n",
      "Epoch 83/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.1027 - mse: 0.0183 - val_loss: 0.1185 - val_mse: 0.0259\n",
      "Epoch 84/200\n",
      "829/829 [==============================] - 16s 19ms/step - loss: 0.1003 - mse: 0.0172 - val_loss: 0.1285 - val_mse: 0.0296\n",
      "Epoch 85/200\n",
      "829/829 [==============================] - 16s 19ms/step - loss: 0.1003 - mse: 0.0176 - val_loss: 0.1278 - val_mse: 0.0287\n",
      "Epoch 86/200\n",
      "829/829 [==============================] - 13s 16ms/step - loss: 0.1009 - mse: 0.0177 - val_loss: 0.1317 - val_mse: 0.0324\n",
      "Epoch 87/200\n",
      "829/829 [==============================] - 14s 17ms/step - loss: 0.1009 - mse: 0.0177 - val_loss: 0.1303 - val_mse: 0.0303\n",
      "Epoch 88/200\n",
      "829/829 [==============================] - 13s 16ms/step - loss: 0.0983 - mse: 0.0169 - val_loss: 0.1184 - val_mse: 0.0261\n",
      "Epoch 89/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.0997 - mse: 0.0171 - val_loss: 0.1210 - val_mse: 0.0268\n",
      "Epoch 90/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.0981 - mse: 0.0167 - val_loss: 0.1360 - val_mse: 0.0336\n",
      "Epoch 91/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.0976 - mse: 0.0166 - val_loss: 0.1351 - val_mse: 0.0327\n",
      "Epoch 92/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.0974 - mse: 0.0165 - val_loss: 0.1391 - val_mse: 0.0341\n",
      "Epoch 93/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.0965 - mse: 0.0163 - val_loss: 0.1228 - val_mse: 0.0281\n",
      "Epoch 94/200\n",
      "829/829 [==============================] - 13s 16ms/step - loss: 0.0981 - mse: 0.0168 - val_loss: 0.1302 - val_mse: 0.0308\n",
      "Epoch 95/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.0965 - mse: 0.0162 - val_loss: 0.1179 - val_mse: 0.0278\n",
      "Epoch 96/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.0963 - mse: 0.0162 - val_loss: 0.1302 - val_mse: 0.0315\n",
      "Epoch 97/200\n",
      "829/829 [==============================] - 13s 16ms/step - loss: 0.0960 - mse: 0.0160 - val_loss: 0.1122 - val_mse: 0.0237\n",
      "Epoch 98/200\n",
      "829/829 [==============================] - 13s 16ms/step - loss: 0.0953 - mse: 0.0157 - val_loss: 0.1229 - val_mse: 0.0279\n",
      "Epoch 99/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.0956 - mse: 0.0159 - val_loss: 0.1218 - val_mse: 0.0267\n",
      "Epoch 100/200\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.0953 - mse: 0.0158 - val_loss: 0.1411 - val_mse: 0.0373\n",
      "Epoch 101/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.0936 - mse: 0.0154 - val_loss: 0.1221 - val_mse: 0.0300\n",
      "Epoch 102/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.0950 - mse: 0.0159 - val_loss: 0.1280 - val_mse: 0.0291\n",
      "Epoch 103/200\n",
      "829/829 [==============================] - 12s 15ms/step - loss: 0.0954 - mse: 0.0160 - val_loss: 0.1231 - val_mse: 0.0277\n",
      "window_len 15 model Model Evaluation - RMSE: 12.15, MAE: 8.83\n",
      "Epoch 1/200\n",
      "808/808 [==============================] - 18s 17ms/step - loss: 0.6148 - mse: 0.5907 - val_loss: 0.6086 - val_mse: 0.5850\n",
      "Epoch 2/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.5950 - mse: 0.5600 - val_loss: 0.5872 - val_mse: 0.5479\n",
      "Epoch 3/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.5740 - mse: 0.5290 - val_loss: 0.5638 - val_mse: 0.5146\n",
      "Epoch 4/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.5289 - mse: 0.4706 - val_loss: 0.4915 - val_mse: 0.4086\n",
      "Epoch 5/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.4750 - mse: 0.3930 - val_loss: 0.4415 - val_mse: 0.3595\n",
      "Epoch 6/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.4277 - mse: 0.3301 - val_loss: 0.4143 - val_mse: 0.2986\n",
      "Epoch 7/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 0.3737 - mse: 0.2610 - val_loss: 0.3543 - val_mse: 0.2444\n",
      "Epoch 8/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.3278 - mse: 0.2043 - val_loss: 0.3165 - val_mse: 0.1963\n",
      "Epoch 9/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 0.2957 - mse: 0.1681 - val_loss: 0.2743 - val_mse: 0.1476\n",
      "Epoch 10/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 0.2646 - mse: 0.1351 - val_loss: 0.2649 - val_mse: 0.1382\n",
      "Epoch 11/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 0.2406 - mse: 0.1110 - val_loss: 0.2184 - val_mse: 0.1021\n",
      "Epoch 12/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.2269 - mse: 0.0992 - val_loss: 0.2140 - val_mse: 0.0901\n",
      "Epoch 13/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.2114 - mse: 0.0849 - val_loss: 0.1862 - val_mse: 0.0701\n",
      "Epoch 14/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.1993 - mse: 0.0750 - val_loss: 0.1875 - val_mse: 0.0708\n",
      "Epoch 15/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.1939 - mse: 0.0694 - val_loss: 0.1708 - val_mse: 0.0578\n",
      "Epoch 16/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.1815 - mse: 0.0609 - val_loss: 0.1623 - val_mse: 0.0548\n",
      "Epoch 17/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.1735 - mse: 0.0557 - val_loss: 0.1643 - val_mse: 0.0537\n",
      "Epoch 18/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.1686 - mse: 0.0521 - val_loss: 0.1988 - val_mse: 0.0713\n",
      "Epoch 19/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.1592 - mse: 0.0468 - val_loss: 0.1647 - val_mse: 0.0502\n",
      "Epoch 20/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 0.1547 - mse: 0.0429 - val_loss: 0.1463 - val_mse: 0.0425\n",
      "Epoch 21/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.1492 - mse: 0.0402 - val_loss: 0.1370 - val_mse: 0.0365\n",
      "Epoch 22/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.1461 - mse: 0.0384 - val_loss: 0.1313 - val_mse: 0.0342\n",
      "Epoch 23/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1441 - mse: 0.0374 - val_loss: 0.1304 - val_mse: 0.0329\n",
      "Epoch 24/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1395 - mse: 0.0346 - val_loss: 0.1168 - val_mse: 0.0276\n",
      "Epoch 25/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1349 - mse: 0.0323 - val_loss: 0.1198 - val_mse: 0.0289\n",
      "Epoch 26/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1312 - mse: 0.0310 - val_loss: 0.1341 - val_mse: 0.0331\n",
      "Epoch 27/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1313 - mse: 0.0310 - val_loss: 0.1324 - val_mse: 0.0337\n",
      "Epoch 28/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1266 - mse: 0.0289 - val_loss: 0.1147 - val_mse: 0.0268\n",
      "Epoch 29/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1249 - mse: 0.0277 - val_loss: 0.1370 - val_mse: 0.0364\n",
      "Epoch 30/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1214 - mse: 0.0260 - val_loss: 0.1208 - val_mse: 0.0268\n",
      "Epoch 31/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1212 - mse: 0.0260 - val_loss: 0.1247 - val_mse: 0.0293\n",
      "Epoch 32/200\n",
      "808/808 [==============================] - 14s 18ms/step - loss: 0.1192 - mse: 0.0253 - val_loss: 0.1260 - val_mse: 0.0304\n",
      "Epoch 33/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.1168 - mse: 0.0242 - val_loss: 0.1177 - val_mse: 0.0268\n",
      "Epoch 34/200\n",
      "808/808 [==============================] - 16s 20ms/step - loss: 0.1142 - mse: 0.0230 - val_loss: 0.1252 - val_mse: 0.0299\n",
      "Epoch 35/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1148 - mse: 0.0235 - val_loss: 0.1105 - val_mse: 0.0248\n",
      "Epoch 36/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.1111 - mse: 0.0216 - val_loss: 0.1058 - val_mse: 0.0208\n",
      "Epoch 37/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.1109 - mse: 0.0219 - val_loss: 0.1074 - val_mse: 0.0219\n",
      "Epoch 38/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.1095 - mse: 0.0213 - val_loss: 0.1045 - val_mse: 0.0199\n",
      "Epoch 39/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.1099 - mse: 0.0214 - val_loss: 0.1251 - val_mse: 0.0297\n",
      "Epoch 40/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.1070 - mse: 0.0204 - val_loss: 0.1101 - val_mse: 0.0244\n",
      "Epoch 41/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.1054 - mse: 0.0195 - val_loss: 0.1057 - val_mse: 0.0207\n",
      "Epoch 42/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.1036 - mse: 0.0189 - val_loss: 0.0992 - val_mse: 0.0194\n",
      "Epoch 43/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.1040 - mse: 0.0190 - val_loss: 0.1056 - val_mse: 0.0209\n",
      "Epoch 44/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1025 - mse: 0.0182 - val_loss: 0.1114 - val_mse: 0.0243\n",
      "Epoch 45/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.1024 - mse: 0.0183 - val_loss: 0.0956 - val_mse: 0.0173\n",
      "Epoch 46/200\n",
      "808/808 [==============================] - 16s 19ms/step - loss: 0.1022 - mse: 0.0186 - val_loss: 0.1060 - val_mse: 0.0205\n",
      "Epoch 47/200\n",
      "808/808 [==============================] - 16s 20ms/step - loss: 0.1002 - mse: 0.0177 - val_loss: 0.1008 - val_mse: 0.0192\n",
      "Epoch 48/200\n",
      "808/808 [==============================] - 16s 20ms/step - loss: 0.0978 - mse: 0.0168 - val_loss: 0.1163 - val_mse: 0.0264\n",
      "Epoch 49/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.0963 - mse: 0.0162 - val_loss: 0.1030 - val_mse: 0.0196\n",
      "Epoch 50/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.0971 - mse: 0.0166 - val_loss: 0.0923 - val_mse: 0.0167\n",
      "Epoch 51/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.0970 - mse: 0.0166 - val_loss: 0.0989 - val_mse: 0.0192\n",
      "Epoch 52/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.0950 - mse: 0.0157 - val_loss: 0.1039 - val_mse: 0.0208\n",
      "Epoch 53/200\n",
      "808/808 [==============================] - 16s 19ms/step - loss: 0.0945 - mse: 0.0157 - val_loss: 0.1169 - val_mse: 0.0243\n",
      "Epoch 54/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.0924 - mse: 0.0150 - val_loss: 0.0949 - val_mse: 0.0165\n",
      "Epoch 55/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.0911 - mse: 0.0144 - val_loss: 0.1007 - val_mse: 0.0199\n",
      "Epoch 56/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.0936 - mse: 0.0152 - val_loss: 0.1158 - val_mse: 0.0240\n",
      "Epoch 57/200\n",
      "808/808 [==============================] - 16s 20ms/step - loss: 0.0921 - mse: 0.0148 - val_loss: 0.0927 - val_mse: 0.0167\n",
      "Epoch 58/200\n",
      "808/808 [==============================] - 16s 19ms/step - loss: 0.0907 - mse: 0.0143 - val_loss: 0.1026 - val_mse: 0.0201\n",
      "Epoch 59/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.0889 - mse: 0.0138 - val_loss: 0.0894 - val_mse: 0.0144\n",
      "Epoch 60/200\n",
      "808/808 [==============================] - 14s 18ms/step - loss: 0.0882 - mse: 0.0137 - val_loss: 0.1037 - val_mse: 0.0210\n",
      "Epoch 61/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0890 - mse: 0.0137 - val_loss: 0.0979 - val_mse: 0.0187\n",
      "Epoch 62/200\n",
      "808/808 [==============================] - 15s 19ms/step - loss: 0.0934 - mse: 0.0171 - val_loss: 0.0973 - val_mse: 0.0188\n",
      "Epoch 63/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.0868 - mse: 0.0131 - val_loss: 0.1025 - val_mse: 0.0194\n",
      "Epoch 64/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0870 - mse: 0.0131 - val_loss: 0.0950 - val_mse: 0.0168\n",
      "Epoch 65/200\n",
      "808/808 [==============================] - 14s 18ms/step - loss: 0.0853 - mse: 0.0127 - val_loss: 0.0933 - val_mse: 0.0165\n",
      "Epoch 66/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0854 - mse: 0.0126 - val_loss: 0.1016 - val_mse: 0.0184\n",
      "Epoch 67/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0859 - mse: 0.0130 - val_loss: 0.1031 - val_mse: 0.0206\n",
      "Epoch 68/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0854 - mse: 0.0126 - val_loss: 0.1063 - val_mse: 0.0193\n",
      "Epoch 69/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0866 - mse: 0.0133 - val_loss: 0.1064 - val_mse: 0.0211\n",
      "Epoch 70/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.0823 - mse: 0.0116 - val_loss: 0.1062 - val_mse: 0.0208\n",
      "Epoch 71/200\n",
      "808/808 [==============================] - 15s 18ms/step - loss: 0.0838 - mse: 0.0121 - val_loss: 0.1237 - val_mse: 0.0300\n",
      "Epoch 72/200\n",
      "808/808 [==============================] - 16s 19ms/step - loss: 0.0830 - mse: 0.0120 - val_loss: 0.0875 - val_mse: 0.0150\n",
      "Epoch 73/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0820 - mse: 0.0115 - val_loss: 0.1242 - val_mse: 0.0288\n",
      "Epoch 74/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0816 - mse: 0.0115 - val_loss: 0.1037 - val_mse: 0.0192\n",
      "Epoch 75/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0807 - mse: 0.0117 - val_loss: 0.1025 - val_mse: 0.0194\n",
      "Epoch 76/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0809 - mse: 0.0115 - val_loss: 0.1075 - val_mse: 0.0224\n",
      "Epoch 77/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0805 - mse: 0.0113 - val_loss: 0.0891 - val_mse: 0.0143\n",
      "Epoch 78/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0817 - mse: 0.0115 - val_loss: 0.1191 - val_mse: 0.0251\n",
      "Epoch 79/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0808 - mse: 0.0113 - val_loss: 0.0957 - val_mse: 0.0178\n",
      "Epoch 80/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0788 - mse: 0.0107 - val_loss: 0.1158 - val_mse: 0.0231\n",
      "Epoch 81/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0791 - mse: 0.0107 - val_loss: 0.0909 - val_mse: 0.0144\n",
      "Epoch 82/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0806 - mse: 0.0112 - val_loss: 0.0981 - val_mse: 0.0169\n",
      "Epoch 83/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0784 - mse: 0.0108 - val_loss: 0.1006 - val_mse: 0.0208\n",
      "Epoch 84/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0774 - mse: 0.0104 - val_loss: 0.1053 - val_mse: 0.0203\n",
      "Epoch 85/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0766 - mse: 0.0101 - val_loss: 0.0891 - val_mse: 0.0156\n",
      "Epoch 86/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0776 - mse: 0.0104 - val_loss: 0.1083 - val_mse: 0.0196\n",
      "Epoch 87/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0775 - mse: 0.0104 - val_loss: 0.1043 - val_mse: 0.0209\n",
      "Epoch 88/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0769 - mse: 0.0103 - val_loss: 0.0872 - val_mse: 0.0142\n",
      "Epoch 89/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0768 - mse: 0.0102 - val_loss: 0.1033 - val_mse: 0.0184\n",
      "Epoch 90/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0764 - mse: 0.0101 - val_loss: 0.0906 - val_mse: 0.0159\n",
      "Epoch 91/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0757 - mse: 0.0099 - val_loss: 0.0930 - val_mse: 0.0160\n",
      "Epoch 92/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0754 - mse: 0.0098 - val_loss: 0.1025 - val_mse: 0.0194\n",
      "Epoch 93/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0752 - mse: 0.0097 - val_loss: 0.1159 - val_mse: 0.0247\n",
      "Epoch 94/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0752 - mse: 0.0099 - val_loss: 0.0936 - val_mse: 0.0163\n",
      "Epoch 95/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0753 - mse: 0.0098 - val_loss: 0.0843 - val_mse: 0.0137\n",
      "Epoch 96/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0749 - mse: 0.0097 - val_loss: 0.1041 - val_mse: 0.0224\n",
      "Epoch 97/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0749 - mse: 0.0100 - val_loss: 0.0981 - val_mse: 0.0175\n",
      "Epoch 98/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0736 - mse: 0.0093 - val_loss: 0.0888 - val_mse: 0.0136\n",
      "Epoch 99/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0738 - mse: 0.0093 - val_loss: 0.1082 - val_mse: 0.0227\n",
      "Epoch 100/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0726 - mse: 0.0091 - val_loss: 0.0979 - val_mse: 0.0169\n",
      "Epoch 101/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0731 - mse: 0.0092 - val_loss: 0.1061 - val_mse: 0.0215\n",
      "Epoch 102/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0733 - mse: 0.0095 - val_loss: 0.1087 - val_mse: 0.0204\n",
      "Epoch 103/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0726 - mse: 0.0091 - val_loss: 0.1070 - val_mse: 0.0213\n",
      "Epoch 104/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0721 - mse: 0.0090 - val_loss: 0.0950 - val_mse: 0.0161\n",
      "Epoch 105/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0713 - mse: 0.0088 - val_loss: 0.0976 - val_mse: 0.0180\n",
      "Epoch 106/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0719 - mse: 0.0093 - val_loss: 0.1070 - val_mse: 0.0213\n",
      "Epoch 107/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0711 - mse: 0.0089 - val_loss: 0.1065 - val_mse: 0.0201\n",
      "Epoch 108/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0703 - mse: 0.0087 - val_loss: 0.0953 - val_mse: 0.0157\n",
      "Epoch 109/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0706 - mse: 0.0085 - val_loss: 0.1033 - val_mse: 0.0215\n",
      "Epoch 110/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0708 - mse: 0.0087 - val_loss: 0.1095 - val_mse: 0.0213\n",
      "Epoch 111/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0703 - mse: 0.0086 - val_loss: 0.1009 - val_mse: 0.0201\n",
      "Epoch 112/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0696 - mse: 0.0084 - val_loss: 0.0985 - val_mse: 0.0159\n",
      "Epoch 113/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0705 - mse: 0.0087 - val_loss: 0.1027 - val_mse: 0.0197\n",
      "Epoch 114/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0697 - mse: 0.0084 - val_loss: 0.0935 - val_mse: 0.0165\n",
      "Epoch 115/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0702 - mse: 0.0084 - val_loss: 0.1044 - val_mse: 0.0190\n",
      "Epoch 116/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0697 - mse: 0.0086 - val_loss: 0.1005 - val_mse: 0.0186\n",
      "Epoch 117/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0689 - mse: 0.0083 - val_loss: 0.1032 - val_mse: 0.0204\n",
      "Epoch 118/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0682 - mse: 0.0081 - val_loss: 0.0989 - val_mse: 0.0177\n",
      "Epoch 119/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0684 - mse: 0.0082 - val_loss: 0.1103 - val_mse: 0.0288\n",
      "Epoch 120/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0694 - mse: 0.0085 - val_loss: 0.0893 - val_mse: 0.0183\n",
      "Epoch 121/200\n",
      "808/808 [==============================] - 13s 16ms/step - loss: 0.0676 - mse: 0.0080 - val_loss: 0.0951 - val_mse: 0.0180\n",
      "Epoch 122/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0683 - mse: 0.0082 - val_loss: 0.1093 - val_mse: 0.0201\n",
      "Epoch 123/200\n",
      "808/808 [==============================] - 14s 17ms/step - loss: 0.0701 - mse: 0.0090 - val_loss: 0.1199 - val_mse: 0.0260\n",
      "Epoch 124/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0681 - mse: 0.0082 - val_loss: 0.0918 - val_mse: 0.0152\n",
      "Epoch 125/200\n",
      "808/808 [==============================] - 13s 17ms/step - loss: 0.0669 - mse: 0.0078 - val_loss: 0.0878 - val_mse: 0.0135\n",
      "window_len 20 model Model Evaluation - RMSE: 10.08, MAE: 6.90\n",
      "Epoch 1/200\n",
      "786/786 [==============================] - 18s 19ms/step - loss: 0.6219 - mse: 0.6052 - val_loss: 0.6188 - val_mse: 0.5912\n",
      "Epoch 2/200\n",
      "786/786 [==============================] - 14s 17ms/step - loss: 0.5970 - mse: 0.5657 - val_loss: 0.5735 - val_mse: 0.5166\n",
      "Epoch 3/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.5601 - mse: 0.5112 - val_loss: 0.5644 - val_mse: 0.5076\n",
      "Epoch 4/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.5074 - mse: 0.4348 - val_loss: 0.4789 - val_mse: 0.3876\n",
      "Epoch 5/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.4466 - mse: 0.3485 - val_loss: 0.4378 - val_mse: 0.3498\n",
      "Epoch 6/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.3875 - mse: 0.2699 - val_loss: 0.3591 - val_mse: 0.2406\n",
      "Epoch 7/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.3312 - mse: 0.2009 - val_loss: 0.2958 - val_mse: 0.1662\n",
      "Epoch 8/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.2874 - mse: 0.1511 - val_loss: 0.2563 - val_mse: 0.1302\n",
      "Epoch 9/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.2564 - mse: 0.1210 - val_loss: 0.2222 - val_mse: 0.0946\n",
      "Epoch 10/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.2322 - mse: 0.0987 - val_loss: 0.2040 - val_mse: 0.0808\n",
      "Epoch 11/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.2129 - mse: 0.0823 - val_loss: 0.2063 - val_mse: 0.0761\n",
      "Epoch 12/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1953 - mse: 0.0686 - val_loss: 0.1943 - val_mse: 0.0702\n",
      "Epoch 13/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1827 - mse: 0.0598 - val_loss: 0.1705 - val_mse: 0.0539\n",
      "Epoch 14/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1694 - mse: 0.0509 - val_loss: 0.1431 - val_mse: 0.0377\n",
      "Epoch 15/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1585 - mse: 0.0440 - val_loss: 0.1654 - val_mse: 0.0448\n",
      "Epoch 16/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1548 - mse: 0.0418 - val_loss: 0.1271 - val_mse: 0.0293\n",
      "Epoch 17/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1496 - mse: 0.0389 - val_loss: 0.1444 - val_mse: 0.0372\n",
      "Epoch 18/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1415 - mse: 0.0349 - val_loss: 0.1170 - val_mse: 0.0249\n",
      "Epoch 19/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1384 - mse: 0.0335 - val_loss: 0.1202 - val_mse: 0.0266\n",
      "Epoch 20/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1334 - mse: 0.0311 - val_loss: 0.1070 - val_mse: 0.0201\n",
      "Epoch 21/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1282 - mse: 0.0289 - val_loss: 0.1212 - val_mse: 0.0252\n",
      "Epoch 22/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1248 - mse: 0.0276 - val_loss: 0.1141 - val_mse: 0.0228\n",
      "Epoch 23/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1232 - mse: 0.0265 - val_loss: 0.1281 - val_mse: 0.0280\n",
      "Epoch 24/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1208 - mse: 0.0255 - val_loss: 0.1115 - val_mse: 0.0223\n",
      "Epoch 25/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1163 - mse: 0.0238 - val_loss: 0.1123 - val_mse: 0.0218\n",
      "Epoch 26/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1125 - mse: 0.0219 - val_loss: 0.1018 - val_mse: 0.0180\n",
      "Epoch 27/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1128 - mse: 0.0222 - val_loss: 0.1285 - val_mse: 0.0269\n",
      "Epoch 28/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1099 - mse: 0.0209 - val_loss: 0.0954 - val_mse: 0.0161\n",
      "Epoch 29/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1087 - mse: 0.0208 - val_loss: 0.1140 - val_mse: 0.0223\n",
      "Epoch 30/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1038 - mse: 0.0188 - val_loss: 0.0897 - val_mse: 0.0136\n",
      "Epoch 31/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1064 - mse: 0.0198 - val_loss: 0.1047 - val_mse: 0.0182\n",
      "Epoch 32/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1017 - mse: 0.0181 - val_loss: 0.0826 - val_mse: 0.0121\n",
      "Epoch 33/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1012 - mse: 0.0181 - val_loss: 0.1221 - val_mse: 0.0238\n",
      "Epoch 34/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.1000 - mse: 0.0172 - val_loss: 0.0901 - val_mse: 0.0148\n",
      "Epoch 35/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0952 - mse: 0.0161 - val_loss: 0.0960 - val_mse: 0.0163\n",
      "Epoch 36/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0955 - mse: 0.0160 - val_loss: 0.0786 - val_mse: 0.0113\n",
      "Epoch 37/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0926 - mse: 0.0151 - val_loss: 0.1034 - val_mse: 0.0183\n",
      "Epoch 38/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0945 - mse: 0.0156 - val_loss: 0.1006 - val_mse: 0.0167\n",
      "Epoch 39/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0927 - mse: 0.0149 - val_loss: 0.0892 - val_mse: 0.0130\n",
      "Epoch 40/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0904 - mse: 0.0144 - val_loss: 0.0892 - val_mse: 0.0152\n",
      "Epoch 41/200\n",
      "786/786 [==============================] - 17s 21ms/step - loss: 0.0882 - mse: 0.0136 - val_loss: 0.0940 - val_mse: 0.0144\n",
      "Epoch 42/200\n",
      "786/786 [==============================] - 15s 19ms/step - loss: 0.0900 - mse: 0.0140 - val_loss: 0.0986 - val_mse: 0.0156\n",
      "Epoch 43/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0887 - mse: 0.0138 - val_loss: 0.0792 - val_mse: 0.0111\n",
      "Epoch 44/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0855 - mse: 0.0131 - val_loss: 0.1024 - val_mse: 0.0169\n",
      "Epoch 45/200\n",
      "786/786 [==============================] - 15s 19ms/step - loss: 0.0852 - mse: 0.0125 - val_loss: 0.0931 - val_mse: 0.0143\n",
      "Epoch 46/200\n",
      "786/786 [==============================] - 15s 19ms/step - loss: 0.0838 - mse: 0.0122 - val_loss: 0.1095 - val_mse: 0.0182\n",
      "Epoch 47/200\n",
      "786/786 [==============================] - 15s 20ms/step - loss: 0.0853 - mse: 0.0127 - val_loss: 0.0967 - val_mse: 0.0159\n",
      "Epoch 48/200\n",
      "786/786 [==============================] - 15s 20ms/step - loss: 0.0816 - mse: 0.0116 - val_loss: 0.0898 - val_mse: 0.0150\n",
      "Epoch 49/200\n",
      "786/786 [==============================] - 15s 19ms/step - loss: 0.0827 - mse: 0.0120 - val_loss: 0.1046 - val_mse: 0.0202\n",
      "Epoch 50/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0801 - mse: 0.0111 - val_loss: 0.0970 - val_mse: 0.0176\n",
      "Epoch 51/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0827 - mse: 0.0118 - val_loss: 0.0706 - val_mse: 0.0087\n",
      "Epoch 52/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0803 - mse: 0.0110 - val_loss: 0.0872 - val_mse: 0.0123\n",
      "Epoch 53/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0794 - mse: 0.0109 - val_loss: 0.0822 - val_mse: 0.0115\n",
      "Epoch 54/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0783 - mse: 0.0108 - val_loss: 0.0913 - val_mse: 0.0138\n",
      "Epoch 55/200\n",
      "786/786 [==============================] - 15s 19ms/step - loss: 0.0779 - mse: 0.0106 - val_loss: 0.0798 - val_mse: 0.0104\n",
      "Epoch 56/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0780 - mse: 0.0107 - val_loss: 0.0988 - val_mse: 0.0165\n",
      "Epoch 57/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0766 - mse: 0.0103 - val_loss: 0.1030 - val_mse: 0.0166\n",
      "Epoch 58/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0747 - mse: 0.0098 - val_loss: 0.0850 - val_mse: 0.0115\n",
      "Epoch 59/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0751 - mse: 0.0098 - val_loss: 0.0917 - val_mse: 0.0141\n",
      "Epoch 60/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0744 - mse: 0.0096 - val_loss: 0.0826 - val_mse: 0.0114\n",
      "Epoch 61/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0739 - mse: 0.0096 - val_loss: 0.0834 - val_mse: 0.0112\n",
      "Epoch 62/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0733 - mse: 0.0094 - val_loss: 0.0932 - val_mse: 0.0156\n",
      "Epoch 63/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0727 - mse: 0.0094 - val_loss: 0.0832 - val_mse: 0.0114\n",
      "Epoch 64/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0723 - mse: 0.0090 - val_loss: 0.0941 - val_mse: 0.0149\n",
      "Epoch 65/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0728 - mse: 0.0092 - val_loss: 0.0922 - val_mse: 0.0143\n",
      "Epoch 66/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0718 - mse: 0.0090 - val_loss: 0.0951 - val_mse: 0.0156\n",
      "Epoch 67/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0695 - mse: 0.0086 - val_loss: 0.0967 - val_mse: 0.0147\n",
      "Epoch 68/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0699 - mse: 0.0085 - val_loss: 0.0934 - val_mse: 0.0153\n",
      "Epoch 69/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0692 - mse: 0.0084 - val_loss: 0.0897 - val_mse: 0.0137\n",
      "Epoch 70/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0688 - mse: 0.0084 - val_loss: 0.0925 - val_mse: 0.0141\n",
      "Epoch 71/200\n",
      "786/786 [==============================] - 15s 19ms/step - loss: 0.0693 - mse: 0.0085 - val_loss: 0.1061 - val_mse: 0.0189\n",
      "Epoch 72/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0689 - mse: 0.0083 - val_loss: 0.0950 - val_mse: 0.0143\n",
      "Epoch 73/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0693 - mse: 0.0083 - val_loss: 0.0826 - val_mse: 0.0111\n",
      "Epoch 74/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0670 - mse: 0.0079 - val_loss: 0.0803 - val_mse: 0.0114\n",
      "Epoch 75/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0670 - mse: 0.0078 - val_loss: 0.0807 - val_mse: 0.0107\n",
      "Epoch 76/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0666 - mse: 0.0078 - val_loss: 0.0729 - val_mse: 0.0091\n",
      "Epoch 77/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0667 - mse: 0.0078 - val_loss: 0.0878 - val_mse: 0.0123\n",
      "Epoch 78/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0658 - mse: 0.0075 - val_loss: 0.0801 - val_mse: 0.0105\n",
      "Epoch 79/200\n",
      "786/786 [==============================] - 15s 19ms/step - loss: 0.0664 - mse: 0.0078 - val_loss: 0.0752 - val_mse: 0.0091\n",
      "Epoch 80/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0653 - mse: 0.0073 - val_loss: 0.1009 - val_mse: 0.0178\n",
      "Epoch 81/200\n",
      "786/786 [==============================] - 14s 18ms/step - loss: 0.0650 - mse: 0.0074 - val_loss: 0.0863 - val_mse: 0.0126\n",
      "window_len 25 model Model Evaluation - RMSE: 7.28, MAE: 5.44\n",
      "Epoch 1/200\n",
      "764/764 [==============================] - 19s 20ms/step - loss: 0.6225 - mse: 0.5993 - val_loss: 0.6003 - val_mse: 0.5475\n",
      "Epoch 2/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.5962 - mse: 0.5593 - val_loss: 0.5663 - val_mse: 0.5296\n",
      "Epoch 3/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.5491 - mse: 0.4941 - val_loss: 0.5051 - val_mse: 0.4258\n",
      "Epoch 4/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.4782 - mse: 0.3903 - val_loss: 0.4176 - val_mse: 0.3009\n",
      "Epoch 5/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.4102 - mse: 0.2957 - val_loss: 0.3683 - val_mse: 0.2309\n",
      "Epoch 6/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.3475 - mse: 0.2190 - val_loss: 0.2984 - val_mse: 0.1665\n",
      "Epoch 7/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.2919 - mse: 0.1572 - val_loss: 0.2682 - val_mse: 0.1235\n",
      "Epoch 8/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.2537 - mse: 0.1192 - val_loss: 0.2117 - val_mse: 0.0838\n",
      "Epoch 9/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.2212 - mse: 0.0923 - val_loss: 0.1930 - val_mse: 0.0676\n",
      "Epoch 10/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.1993 - mse: 0.0750 - val_loss: 0.1924 - val_mse: 0.0687\n",
      "Epoch 11/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1855 - mse: 0.0648 - val_loss: 0.1700 - val_mse: 0.0510\n",
      "Epoch 12/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1677 - mse: 0.0528 - val_loss: 0.1950 - val_mse: 0.0630\n",
      "Epoch 13/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1603 - mse: 0.0468 - val_loss: 0.1526 - val_mse: 0.0415\n",
      "Epoch 14/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1494 - mse: 0.0404 - val_loss: 0.1311 - val_mse: 0.0302\n",
      "Epoch 15/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1417 - mse: 0.0362 - val_loss: 0.1148 - val_mse: 0.0241\n",
      "Epoch 16/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1342 - mse: 0.0320 - val_loss: 0.1134 - val_mse: 0.0224\n",
      "Epoch 17/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1296 - mse: 0.0301 - val_loss: 0.1035 - val_mse: 0.0182\n",
      "Epoch 18/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1232 - mse: 0.0271 - val_loss: 0.1209 - val_mse: 0.0257\n",
      "Epoch 19/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1202 - mse: 0.0256 - val_loss: 0.1007 - val_mse: 0.0177\n",
      "Epoch 20/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1148 - mse: 0.0237 - val_loss: 0.1013 - val_mse: 0.0189\n",
      "Epoch 21/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1115 - mse: 0.0224 - val_loss: 0.1028 - val_mse: 0.0194\n",
      "Epoch 22/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1096 - mse: 0.0214 - val_loss: 0.0912 - val_mse: 0.0145\n",
      "Epoch 23/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1049 - mse: 0.0199 - val_loss: 0.1006 - val_mse: 0.0189\n",
      "Epoch 24/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1051 - mse: 0.0199 - val_loss: 0.0926 - val_mse: 0.0152\n",
      "Epoch 25/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.1001 - mse: 0.0178 - val_loss: 0.0934 - val_mse: 0.0149\n",
      "Epoch 26/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0988 - mse: 0.0174 - val_loss: 0.1020 - val_mse: 0.0176\n",
      "Epoch 27/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0964 - mse: 0.0166 - val_loss: 0.0887 - val_mse: 0.0128\n",
      "Epoch 28/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0969 - mse: 0.0167 - val_loss: 0.0848 - val_mse: 0.0119\n",
      "Epoch 29/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0925 - mse: 0.0151 - val_loss: 0.1129 - val_mse: 0.0212\n",
      "Epoch 30/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0912 - mse: 0.0150 - val_loss: 0.1175 - val_mse: 0.0242\n",
      "Epoch 31/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0887 - mse: 0.0138 - val_loss: 0.1224 - val_mse: 0.0240\n",
      "Epoch 32/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0861 - mse: 0.0134 - val_loss: 0.1301 - val_mse: 0.0272\n",
      "Epoch 33/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0858 - mse: 0.0133 - val_loss: 0.0881 - val_mse: 0.0133\n",
      "Epoch 34/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0847 - mse: 0.0129 - val_loss: 0.0835 - val_mse: 0.0132\n",
      "Epoch 35/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0837 - mse: 0.0125 - val_loss: 0.0770 - val_mse: 0.0104\n",
      "Epoch 36/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0828 - mse: 0.0124 - val_loss: 0.0881 - val_mse: 0.0139\n",
      "Epoch 37/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0817 - mse: 0.0120 - val_loss: 0.0675 - val_mse: 0.0085\n",
      "Epoch 38/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0788 - mse: 0.0111 - val_loss: 0.0773 - val_mse: 0.0106\n",
      "Epoch 39/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0771 - mse: 0.0106 - val_loss: 0.0787 - val_mse: 0.0104\n",
      "Epoch 40/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0778 - mse: 0.0110 - val_loss: 0.0802 - val_mse: 0.0109\n",
      "Epoch 41/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0763 - mse: 0.0104 - val_loss: 0.0971 - val_mse: 0.0150\n",
      "Epoch 42/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0740 - mse: 0.0097 - val_loss: 0.0855 - val_mse: 0.0132\n",
      "Epoch 43/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0740 - mse: 0.0099 - val_loss: 0.0837 - val_mse: 0.0121\n",
      "Epoch 44/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0744 - mse: 0.0101 - val_loss: 0.0803 - val_mse: 0.0116\n",
      "Epoch 45/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0719 - mse: 0.0095 - val_loss: 0.0838 - val_mse: 0.0131\n",
      "Epoch 46/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0723 - mse: 0.0093 - val_loss: 0.1070 - val_mse: 0.0174\n",
      "Epoch 47/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0709 - mse: 0.0090 - val_loss: 0.0924 - val_mse: 0.0151\n",
      "Epoch 48/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.0695 - mse: 0.0086 - val_loss: 0.1048 - val_mse: 0.0175\n",
      "Epoch 49/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0695 - mse: 0.0087 - val_loss: 0.0847 - val_mse: 0.0116\n",
      "Epoch 50/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0724 - mse: 0.0105 - val_loss: 0.1054 - val_mse: 0.0197\n",
      "Epoch 51/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0680 - mse: 0.0083 - val_loss: 0.0757 - val_mse: 0.0094\n",
      "Epoch 52/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0677 - mse: 0.0083 - val_loss: 0.0832 - val_mse: 0.0112\n",
      "Epoch 53/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.0653 - mse: 0.0077 - val_loss: 0.1144 - val_mse: 0.0194\n",
      "Epoch 54/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0658 - mse: 0.0077 - val_loss: 0.0912 - val_mse: 0.0133\n",
      "Epoch 55/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0665 - mse: 0.0080 - val_loss: 0.0851 - val_mse: 0.0116\n",
      "Epoch 56/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0659 - mse: 0.0078 - val_loss: 0.0791 - val_mse: 0.0108\n",
      "Epoch 57/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0643 - mse: 0.0075 - val_loss: 0.1012 - val_mse: 0.0174\n",
      "Epoch 58/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.0649 - mse: 0.0076 - val_loss: 0.0941 - val_mse: 0.0145\n",
      "Epoch 59/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0648 - mse: 0.0075 - val_loss: 0.0939 - val_mse: 0.0143\n",
      "Epoch 60/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0641 - mse: 0.0074 - val_loss: 0.0898 - val_mse: 0.0138\n",
      "Epoch 61/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0617 - mse: 0.0069 - val_loss: 0.0954 - val_mse: 0.0136\n",
      "Epoch 62/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.0619 - mse: 0.0069 - val_loss: 0.1079 - val_mse: 0.0190\n",
      "Epoch 63/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0609 - mse: 0.0069 - val_loss: 0.1186 - val_mse: 0.0227\n",
      "Epoch 64/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.0607 - mse: 0.0066 - val_loss: 0.0837 - val_mse: 0.0116\n",
      "Epoch 65/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0614 - mse: 0.0069 - val_loss: 0.0882 - val_mse: 0.0123\n",
      "Epoch 66/200\n",
      "764/764 [==============================] - 14s 18ms/step - loss: 0.0597 - mse: 0.0064 - val_loss: 0.0999 - val_mse: 0.0153\n",
      "Epoch 67/200\n",
      "764/764 [==============================] - 14s 19ms/step - loss: 0.0593 - mse: 0.0065 - val_loss: 0.0764 - val_mse: 0.0099\n",
      "window_len 30 model Model Evaluation - RMSE: 7.17, MAE: 5.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\.conda\\envs\\mlEnv_2\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1020: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(self.var_), copy=False, constant_mask=constant_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "743/743 [==============================] - 18s 20ms/step - loss: 0.6223 - mse: 0.6048 - val_loss: 0.6108 - val_mse: 0.5854\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.5983 - mse: 0.5666 - val_loss: 0.5661 - val_mse: 0.5216\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.5505 - mse: 0.4995 - val_loss: 0.5173 - val_mse: 0.4743\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.4800 - mse: 0.3946 - val_loss: 0.4386 - val_mse: 0.3495\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.4115 - mse: 0.3030 - val_loss: 0.3724 - val_mse: 0.2743\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.3469 - mse: 0.2268 - val_loss: 0.3450 - val_mse: 0.2265\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.2902 - mse: 0.1634 - val_loss: 0.2540 - val_mse: 0.1288\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.2486 - mse: 0.1173 - val_loss: 0.2063 - val_mse: 0.0917\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.2105 - mse: 0.0851 - val_loss: 0.2185 - val_mse: 0.0954\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1888 - mse: 0.0679 - val_loss: 0.1549 - val_mse: 0.0492\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1711 - mse: 0.0545 - val_loss: 0.1393 - val_mse: 0.0386\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1571 - mse: 0.0457 - val_loss: 0.1484 - val_mse: 0.0408\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1456 - mse: 0.0391 - val_loss: 0.1450 - val_mse: 0.0385\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1386 - mse: 0.0346 - val_loss: 0.1211 - val_mse: 0.0270\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1304 - mse: 0.0308 - val_loss: 0.1242 - val_mse: 0.0289\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1255 - mse: 0.0287 - val_loss: 0.1055 - val_mse: 0.0210\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1189 - mse: 0.0255 - val_loss: 0.1164 - val_mse: 0.0235\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1167 - mse: 0.0243 - val_loss: 0.0985 - val_mse: 0.0168\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1117 - mse: 0.0221 - val_loss: 0.1299 - val_mse: 0.0279\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1082 - mse: 0.0209 - val_loss: 0.0874 - val_mse: 0.0132\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1051 - mse: 0.0198 - val_loss: 0.1088 - val_mse: 0.0207\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.1010 - mse: 0.0181 - val_loss: 0.1000 - val_mse: 0.0175\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0990 - mse: 0.0171 - val_loss: 0.1073 - val_mse: 0.0199\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0962 - mse: 0.0164 - val_loss: 0.0830 - val_mse: 0.0118\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0946 - mse: 0.0160 - val_loss: 0.0894 - val_mse: 0.0145\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0943 - mse: 0.0154 - val_loss: 0.1006 - val_mse: 0.0175\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0915 - mse: 0.0148 - val_loss: 0.0681 - val_mse: 0.0085\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0885 - mse: 0.0138 - val_loss: 0.0805 - val_mse: 0.0111\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0875 - mse: 0.0136 - val_loss: 0.0734 - val_mse: 0.0097\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0863 - mse: 0.0131 - val_loss: 0.0913 - val_mse: 0.0139\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0837 - mse: 0.0127 - val_loss: 0.0769 - val_mse: 0.0103\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0823 - mse: 0.0122 - val_loss: 0.0771 - val_mse: 0.0101\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0814 - mse: 0.0119 - val_loss: 0.0791 - val_mse: 0.0107\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0797 - mse: 0.0114 - val_loss: 0.0744 - val_mse: 0.0100\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0792 - mse: 0.0112 - val_loss: 0.0934 - val_mse: 0.0146\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0777 - mse: 0.0109 - val_loss: 0.0703 - val_mse: 0.0085\n",
      "Epoch 37/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0760 - mse: 0.0106 - val_loss: 0.0824 - val_mse: 0.0113\n",
      "Epoch 38/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0738 - mse: 0.0097 - val_loss: 0.0840 - val_mse: 0.0122\n",
      "Epoch 39/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0744 - mse: 0.0100 - val_loss: 0.0728 - val_mse: 0.0089\n",
      "Epoch 40/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0727 - mse: 0.0095 - val_loss: 0.0817 - val_mse: 0.0121\n",
      "Epoch 41/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0735 - mse: 0.0099 - val_loss: 0.0857 - val_mse: 0.0123\n",
      "Epoch 42/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0703 - mse: 0.0089 - val_loss: 0.0832 - val_mse: 0.0125\n",
      "Epoch 43/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0694 - mse: 0.0086 - val_loss: 0.0778 - val_mse: 0.0104\n",
      "Epoch 44/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0701 - mse: 0.0090 - val_loss: 0.0639 - val_mse: 0.0069\n",
      "Epoch 45/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0677 - mse: 0.0083 - val_loss: 0.0757 - val_mse: 0.0099\n",
      "Epoch 46/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0676 - mse: 0.0082 - val_loss: 0.0732 - val_mse: 0.0087\n",
      "Epoch 47/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0672 - mse: 0.0082 - val_loss: 0.0764 - val_mse: 0.0100\n",
      "Epoch 48/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0672 - mse: 0.0081 - val_loss: 0.0937 - val_mse: 0.0161\n",
      "Epoch 49/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0648 - mse: 0.0077 - val_loss: 0.0819 - val_mse: 0.0108\n",
      "Epoch 50/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0642 - mse: 0.0073 - val_loss: 0.0898 - val_mse: 0.0137\n",
      "Epoch 51/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0644 - mse: 0.0076 - val_loss: 0.0714 - val_mse: 0.0090\n",
      "Epoch 52/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0633 - mse: 0.0073 - val_loss: 0.0850 - val_mse: 0.0124\n",
      "Epoch 53/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0637 - mse: 0.0073 - val_loss: 0.0785 - val_mse: 0.0112\n",
      "Epoch 54/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0627 - mse: 0.0071 - val_loss: 0.0724 - val_mse: 0.0096\n",
      "Epoch 55/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0612 - mse: 0.0068 - val_loss: 0.1126 - val_mse: 0.0205\n",
      "Epoch 56/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0597 - mse: 0.0064 - val_loss: 0.0641 - val_mse: 0.0078\n",
      "Epoch 57/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0604 - mse: 0.0066 - val_loss: 0.0736 - val_mse: 0.0092\n",
      "Epoch 58/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0596 - mse: 0.0065 - val_loss: 0.0732 - val_mse: 0.0093\n",
      "Epoch 59/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0596 - mse: 0.0066 - val_loss: 0.0896 - val_mse: 0.0128\n",
      "Epoch 60/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0590 - mse: 0.0062 - val_loss: 0.0747 - val_mse: 0.0099\n",
      "Epoch 61/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0584 - mse: 0.0062 - val_loss: 0.0648 - val_mse: 0.0080\n",
      "Epoch 62/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0586 - mse: 0.0062 - val_loss: 0.0716 - val_mse: 0.0100\n",
      "Epoch 63/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0564 - mse: 0.0057 - val_loss: 0.0685 - val_mse: 0.0082\n",
      "Epoch 64/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0572 - mse: 0.0060 - val_loss: 0.0967 - val_mse: 0.0170\n",
      "Epoch 65/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0564 - mse: 0.0058 - val_loss: 0.0623 - val_mse: 0.0069\n",
      "Epoch 66/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0566 - mse: 0.0059 - val_loss: 0.0673 - val_mse: 0.0081\n",
      "Epoch 67/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0553 - mse: 0.0055 - val_loss: 0.0889 - val_mse: 0.0142\n",
      "Epoch 68/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0541 - mse: 0.0053 - val_loss: 0.0801 - val_mse: 0.0118\n",
      "Epoch 69/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0540 - mse: 0.0053 - val_loss: 0.0723 - val_mse: 0.0093\n",
      "Epoch 70/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0538 - mse: 0.0052 - val_loss: 0.0688 - val_mse: 0.0082\n",
      "Epoch 71/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0539 - mse: 0.0053 - val_loss: 0.0739 - val_mse: 0.0093\n",
      "Epoch 72/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0528 - mse: 0.0052 - val_loss: 0.0622 - val_mse: 0.0066\n",
      "Epoch 73/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0529 - mse: 0.0051 - val_loss: 0.0702 - val_mse: 0.0094\n",
      "Epoch 74/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0521 - mse: 0.0049 - val_loss: 0.0856 - val_mse: 0.0123\n",
      "Epoch 75/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0524 - mse: 0.0050 - val_loss: 0.0779 - val_mse: 0.0104\n",
      "Epoch 76/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0510 - mse: 0.0047 - val_loss: 0.0732 - val_mse: 0.0091\n",
      "Epoch 77/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0510 - mse: 0.0049 - val_loss: 0.0679 - val_mse: 0.0088\n",
      "Epoch 78/200\n",
      "743/743 [==============================] - 14s 19ms/step - loss: 0.0501 - mse: 0.0046 - val_loss: 0.0601 - val_mse: 0.0066\n",
      "Epoch 79/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0500 - mse: 0.0046 - val_loss: 0.0829 - val_mse: 0.0124\n",
      "Epoch 80/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0511 - mse: 0.0049 - val_loss: 0.0533 - val_mse: 0.0057\n",
      "Epoch 81/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0490 - mse: 0.0044 - val_loss: 0.0688 - val_mse: 0.0076\n",
      "Epoch 82/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0495 - mse: 0.0045 - val_loss: 0.0818 - val_mse: 0.0119\n",
      "Epoch 83/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0488 - mse: 0.0044 - val_loss: 0.0770 - val_mse: 0.0094\n",
      "Epoch 84/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0502 - mse: 0.0054 - val_loss: 0.0683 - val_mse: 0.0082\n",
      "Epoch 85/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0490 - mse: 0.0044 - val_loss: 0.0656 - val_mse: 0.0075\n",
      "Epoch 86/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0480 - mse: 0.0044 - val_loss: 0.0624 - val_mse: 0.0069\n",
      "Epoch 87/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0486 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0119\n",
      "Epoch 88/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0468 - mse: 0.0040 - val_loss: 0.0693 - val_mse: 0.0086\n",
      "Epoch 89/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0475 - mse: 0.0042 - val_loss: 0.0758 - val_mse: 0.0100\n",
      "Epoch 90/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0476 - mse: 0.0042 - val_loss: 0.0887 - val_mse: 0.0120\n",
      "Epoch 91/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0474 - mse: 0.0041 - val_loss: 0.0732 - val_mse: 0.0085\n",
      "Epoch 92/200\n",
      "743/743 [==============================] - 14s 18ms/step - loss: 0.0471 - mse: 0.0043 - val_loss: 0.0888 - val_mse: 0.0129\n",
      "Epoch 93/200\n",
      "743/743 [==============================] - 14s 18ms/step - loss: 0.0467 - mse: 0.0041 - val_loss: 0.1034 - val_mse: 0.0173\n",
      "Epoch 94/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0459 - mse: 0.0040 - val_loss: 0.0760 - val_mse: 0.0097\n",
      "Epoch 95/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0460 - mse: 0.0040 - val_loss: 0.0733 - val_mse: 0.0091\n",
      "Epoch 96/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0460 - mse: 0.0039 - val_loss: 0.0682 - val_mse: 0.0082\n",
      "Epoch 97/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0453 - mse: 0.0038 - val_loss: 0.0766 - val_mse: 0.0097\n",
      "Epoch 98/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0450 - mse: 0.0037 - val_loss: 0.0546 - val_mse: 0.0062\n",
      "Epoch 99/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0448 - mse: 0.0037 - val_loss: 0.0638 - val_mse: 0.0073\n",
      "Epoch 100/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0435 - mse: 0.0034 - val_loss: 0.0805 - val_mse: 0.0108\n",
      "Epoch 101/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0446 - mse: 0.0037 - val_loss: 0.0671 - val_mse: 0.0075\n",
      "Epoch 102/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0434 - mse: 0.0036 - val_loss: 0.0674 - val_mse: 0.0080\n",
      "Epoch 103/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0429 - mse: 0.0035 - val_loss: 0.0678 - val_mse: 0.0081\n",
      "Epoch 104/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0440 - mse: 0.0036 - val_loss: 0.0850 - val_mse: 0.0121\n",
      "Epoch 105/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0437 - mse: 0.0036 - val_loss: 0.0665 - val_mse: 0.0086\n",
      "Epoch 106/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0431 - mse: 0.0035 - val_loss: 0.0807 - val_mse: 0.0118\n",
      "Epoch 107/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0431 - mse: 0.0036 - val_loss: 0.0569 - val_mse: 0.0066\n",
      "Epoch 108/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0421 - mse: 0.0033 - val_loss: 0.0802 - val_mse: 0.0106\n",
      "Epoch 109/200\n",
      "743/743 [==============================] - 14s 18ms/step - loss: 0.0429 - mse: 0.0035 - val_loss: 0.0757 - val_mse: 0.0104\n",
      "Epoch 110/200\n",
      "743/743 [==============================] - 13s 18ms/step - loss: 0.0414 - mse: 0.0032 - val_loss: 0.0813 - val_mse: 0.0125\n",
      "window_len 35 model Model Evaluation - RMSE: 5.67, MAE: 4.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\.conda\\envs\\mlEnv_2\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1020: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(self.var_), copy=False, constant_mask=constant_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "721/721 [==============================] - 17s 20ms/step - loss: 0.6241 - mse: 0.6021 - val_loss: 0.5930 - val_mse: 0.5524\n",
      "Epoch 2/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.6009 - mse: 0.5730 - val_loss: 0.5608 - val_mse: 0.5164\n",
      "Epoch 3/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.5588 - mse: 0.5144 - val_loss: 0.5110 - val_mse: 0.4480\n",
      "Epoch 4/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.4690 - mse: 0.3854 - val_loss: 0.3867 - val_mse: 0.2626\n",
      "Epoch 5/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.3753 - mse: 0.2524 - val_loss: 0.3530 - val_mse: 0.2161\n",
      "Epoch 6/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.2967 - mse: 0.1622 - val_loss: 0.2346 - val_mse: 0.1058\n",
      "Epoch 7/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.2401 - mse: 0.1080 - val_loss: 0.2109 - val_mse: 0.0773\n",
      "Epoch 8/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.2010 - mse: 0.0749 - val_loss: 0.1578 - val_mse: 0.0487\n",
      "Epoch 9/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.1775 - mse: 0.0583 - val_loss: 0.1491 - val_mse: 0.0405\n",
      "Epoch 10/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.1579 - mse: 0.0460 - val_loss: 0.1347 - val_mse: 0.0328\n",
      "Epoch 11/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.1524 - mse: 0.0421 - val_loss: 0.1131 - val_mse: 0.0241\n",
      "Epoch 12/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.1432 - mse: 0.0369 - val_loss: 0.1053 - val_mse: 0.0213\n",
      "Epoch 13/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.1340 - mse: 0.0325 - val_loss: 0.1000 - val_mse: 0.0185\n",
      "Epoch 14/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.1260 - mse: 0.0284 - val_loss: 0.0927 - val_mse: 0.0162\n",
      "Epoch 15/200\n",
      "721/721 [==============================] - 13s 18ms/step - loss: 0.1193 - mse: 0.0258 - val_loss: 0.0997 - val_mse: 0.0172\n",
      "Epoch 16/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.1153 - mse: 0.0241 - val_loss: 0.1176 - val_mse: 0.0243\n",
      "Epoch 17/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.1145 - mse: 0.0233 - val_loss: 0.0820 - val_mse: 0.0128\n",
      "Epoch 18/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.1095 - mse: 0.0211 - val_loss: 0.0959 - val_mse: 0.0156\n",
      "Epoch 19/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.1066 - mse: 0.0205 - val_loss: 0.0840 - val_mse: 0.0134\n",
      "Epoch 20/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.1048 - mse: 0.0196 - val_loss: 0.0812 - val_mse: 0.0126\n",
      "Epoch 21/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0974 - mse: 0.0170 - val_loss: 0.0869 - val_mse: 0.0128\n",
      "Epoch 22/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0951 - mse: 0.0161 - val_loss: 0.0680 - val_mse: 0.0084\n",
      "Epoch 23/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.0926 - mse: 0.0153 - val_loss: 0.0947 - val_mse: 0.0153\n",
      "Epoch 24/200\n",
      "721/721 [==============================] - 13s 18ms/step - loss: 0.0907 - mse: 0.0148 - val_loss: 0.0973 - val_mse: 0.0169\n",
      "Epoch 25/200\n",
      "721/721 [==============================] - 13s 19ms/step - loss: 0.0900 - mse: 0.0145 - val_loss: 0.0988 - val_mse: 0.0159\n",
      "Epoch 26/200\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.0862 - mse: 0.0134 - val_loss: 0.0800 - val_mse: 0.0118\n",
      "Epoch 27/200\n",
      "721/721 [==============================] - 15s 21ms/step - loss: 0.0851 - mse: 0.0128 - val_loss: 0.0656 - val_mse: 0.0081\n",
      "Epoch 28/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0839 - mse: 0.0124 - val_loss: 0.0697 - val_mse: 0.0083\n",
      "Epoch 29/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0825 - mse: 0.0124 - val_loss: 0.0707 - val_mse: 0.0086\n",
      "Epoch 30/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0807 - mse: 0.0116 - val_loss: 0.0652 - val_mse: 0.0077\n",
      "Epoch 31/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0806 - mse: 0.0120 - val_loss: 0.0785 - val_mse: 0.0106\n",
      "Epoch 32/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0790 - mse: 0.0112 - val_loss: 0.0944 - val_mse: 0.0152\n",
      "Epoch 33/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0775 - mse: 0.0108 - val_loss: 0.0762 - val_mse: 0.0099\n",
      "Epoch 34/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0756 - mse: 0.0101 - val_loss: 0.0794 - val_mse: 0.0118\n",
      "Epoch 35/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0738 - mse: 0.0097 - val_loss: 0.0781 - val_mse: 0.0111\n",
      "Epoch 36/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0731 - mse: 0.0096 - val_loss: 0.0791 - val_mse: 0.0101\n",
      "Epoch 37/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0726 - mse: 0.0098 - val_loss: 0.0640 - val_mse: 0.0069\n",
      "Epoch 38/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0713 - mse: 0.0093 - val_loss: 0.0832 - val_mse: 0.0115\n",
      "Epoch 39/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0704 - mse: 0.0090 - val_loss: 0.0668 - val_mse: 0.0075\n",
      "Epoch 40/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0697 - mse: 0.0089 - val_loss: 0.0715 - val_mse: 0.0094\n",
      "Epoch 41/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0678 - mse: 0.0082 - val_loss: 0.0782 - val_mse: 0.0110\n",
      "Epoch 42/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0670 - mse: 0.0080 - val_loss: 0.0694 - val_mse: 0.0088\n",
      "Epoch 43/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0672 - mse: 0.0081 - val_loss: 0.1084 - val_mse: 0.0195\n",
      "Epoch 44/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0649 - mse: 0.0076 - val_loss: 0.0890 - val_mse: 0.0137\n",
      "Epoch 45/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0644 - mse: 0.0077 - val_loss: 0.0487 - val_mse: 0.0045\n",
      "Epoch 46/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0647 - mse: 0.0076 - val_loss: 0.0787 - val_mse: 0.0117\n",
      "Epoch 47/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0625 - mse: 0.0069 - val_loss: 0.0946 - val_mse: 0.0142\n",
      "Epoch 48/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0620 - mse: 0.0071 - val_loss: 0.0714 - val_mse: 0.0099\n",
      "Epoch 49/200\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.0632 - mse: 0.0073 - val_loss: 0.0609 - val_mse: 0.0065\n",
      "Epoch 50/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0618 - mse: 0.0071 - val_loss: 0.0779 - val_mse: 0.0101\n",
      "Epoch 51/200\n",
      "721/721 [==============================] - 16s 23ms/step - loss: 0.0606 - mse: 0.0068 - val_loss: 0.0735 - val_mse: 0.0095\n",
      "Epoch 52/200\n",
      "721/721 [==============================] - 17s 23ms/step - loss: 0.0597 - mse: 0.0064 - val_loss: 0.0701 - val_mse: 0.0094\n",
      "Epoch 53/200\n",
      "721/721 [==============================] - 16s 23ms/step - loss: 0.0596 - mse: 0.0064 - val_loss: 0.0837 - val_mse: 0.0138\n",
      "Epoch 54/200\n",
      "721/721 [==============================] - 16s 22ms/step - loss: 0.0580 - mse: 0.0062 - val_loss: 0.0606 - val_mse: 0.0065\n",
      "Epoch 55/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0591 - mse: 0.0064 - val_loss: 0.0667 - val_mse: 0.0078\n",
      "Epoch 56/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0586 - mse: 0.0062 - val_loss: 0.0892 - val_mse: 0.0138\n",
      "Epoch 57/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0569 - mse: 0.0060 - val_loss: 0.0649 - val_mse: 0.0068\n",
      "Epoch 58/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0565 - mse: 0.0059 - val_loss: 0.0818 - val_mse: 0.0121\n",
      "Epoch 59/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0553 - mse: 0.0056 - val_loss: 0.0694 - val_mse: 0.0079\n",
      "Epoch 60/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0562 - mse: 0.0057 - val_loss: 0.0689 - val_mse: 0.0083\n",
      "Epoch 61/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0553 - mse: 0.0057 - val_loss: 0.0886 - val_mse: 0.0127\n",
      "Epoch 62/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0546 - mse: 0.0054 - val_loss: 0.0807 - val_mse: 0.0129\n",
      "Epoch 63/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0541 - mse: 0.0052 - val_loss: 0.0857 - val_mse: 0.0128\n",
      "Epoch 64/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0532 - mse: 0.0051 - val_loss: 0.0888 - val_mse: 0.0137\n",
      "Epoch 65/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0544 - mse: 0.0054 - val_loss: 0.0617 - val_mse: 0.0065\n",
      "Epoch 66/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0529 - mse: 0.0051 - val_loss: 0.0776 - val_mse: 0.0104\n",
      "Epoch 67/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0520 - mse: 0.0052 - val_loss: 0.0733 - val_mse: 0.0091\n",
      "Epoch 68/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0514 - mse: 0.0049 - val_loss: 0.0723 - val_mse: 0.0085\n",
      "Epoch 69/200\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.0520 - mse: 0.0049 - val_loss: 0.1013 - val_mse: 0.0189\n",
      "Epoch 70/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0505 - mse: 0.0047 - val_loss: 0.0819 - val_mse: 0.0106\n",
      "Epoch 71/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0503 - mse: 0.0047 - val_loss: 0.0781 - val_mse: 0.0114\n",
      "Epoch 72/200\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.0507 - mse: 0.0047 - val_loss: 0.0676 - val_mse: 0.0073\n",
      "Epoch 73/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0500 - mse: 0.0047 - val_loss: 0.0932 - val_mse: 0.0134\n",
      "Epoch 74/200\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.0496 - mse: 0.0046 - val_loss: 0.0889 - val_mse: 0.0134\n",
      "Epoch 75/200\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.0489 - mse: 0.0044 - val_loss: 0.0845 - val_mse: 0.0112\n",
      "window_len 40 model Model Evaluation - RMSE: 4.88, MAE: 3.63\n",
      "Epoch 1/200\n",
      "699/699 [==============================] - 19s 22ms/step - loss: 0.6323 - mse: 0.6252 - val_loss: 0.6189 - val_mse: 0.5957\n",
      "Epoch 2/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.5811 - mse: 0.5432 - val_loss: 0.5166 - val_mse: 0.4636\n",
      "Epoch 3/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.4945 - mse: 0.4256 - val_loss: 0.4253 - val_mse: 0.3216\n",
      "Epoch 4/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.3860 - mse: 0.2759 - val_loss: 0.3308 - val_mse: 0.2242\n",
      "Epoch 5/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.3079 - mse: 0.1825 - val_loss: 0.2569 - val_mse: 0.1380\n",
      "Epoch 6/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.2530 - mse: 0.1230 - val_loss: 0.2565 - val_mse: 0.1318\n",
      "Epoch 7/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.2148 - mse: 0.0869 - val_loss: 0.1791 - val_mse: 0.0620\n",
      "Epoch 8/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1867 - mse: 0.0647 - val_loss: 0.1679 - val_mse: 0.0521\n",
      "Epoch 9/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1670 - mse: 0.0511 - val_loss: 0.1346 - val_mse: 0.0321\n",
      "Epoch 10/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1554 - mse: 0.0437 - val_loss: 0.1438 - val_mse: 0.0368\n",
      "Epoch 11/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1424 - mse: 0.0360 - val_loss: 0.1208 - val_mse: 0.0262\n",
      "Epoch 12/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1354 - mse: 0.0332 - val_loss: 0.1146 - val_mse: 0.0236\n",
      "Epoch 13/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1306 - mse: 0.0307 - val_loss: 0.1074 - val_mse: 0.0209\n",
      "Epoch 14/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1229 - mse: 0.0270 - val_loss: 0.1168 - val_mse: 0.0222\n",
      "Epoch 15/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1180 - mse: 0.0244 - val_loss: 0.0881 - val_mse: 0.0145\n",
      "Epoch 16/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1145 - mse: 0.0238 - val_loss: 0.0891 - val_mse: 0.0141\n",
      "Epoch 17/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1090 - mse: 0.0215 - val_loss: 0.0882 - val_mse: 0.0137\n",
      "Epoch 18/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1045 - mse: 0.0195 - val_loss: 0.1196 - val_mse: 0.0238\n",
      "Epoch 19/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.1011 - mse: 0.0185 - val_loss: 0.0856 - val_mse: 0.0139\n",
      "Epoch 20/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0973 - mse: 0.0170 - val_loss: 0.0789 - val_mse: 0.0114\n",
      "Epoch 21/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0974 - mse: 0.0170 - val_loss: 0.0913 - val_mse: 0.0137\n",
      "Epoch 22/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0938 - mse: 0.0158 - val_loss: 0.0830 - val_mse: 0.0130\n",
      "Epoch 23/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0920 - mse: 0.0151 - val_loss: 0.0918 - val_mse: 0.0140\n",
      "Epoch 24/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0897 - mse: 0.0144 - val_loss: 0.0839 - val_mse: 0.0125\n",
      "Epoch 25/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0864 - mse: 0.0135 - val_loss: 0.0849 - val_mse: 0.0125\n",
      "Epoch 26/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0853 - mse: 0.0132 - val_loss: 0.0725 - val_mse: 0.0087\n",
      "Epoch 27/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0843 - mse: 0.0127 - val_loss: 0.0754 - val_mse: 0.0103\n",
      "Epoch 28/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0822 - mse: 0.0121 - val_loss: 0.0729 - val_mse: 0.0094\n",
      "Epoch 29/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0800 - mse: 0.0115 - val_loss: 0.0726 - val_mse: 0.0090\n",
      "Epoch 30/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0782 - mse: 0.0110 - val_loss: 0.0903 - val_mse: 0.0137\n",
      "Epoch 31/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0777 - mse: 0.0110 - val_loss: 0.0547 - val_mse: 0.0052\n",
      "Epoch 32/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0759 - mse: 0.0106 - val_loss: 0.0770 - val_mse: 0.0092\n",
      "Epoch 33/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0745 - mse: 0.0100 - val_loss: 0.0828 - val_mse: 0.0107\n",
      "Epoch 34/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0727 - mse: 0.0096 - val_loss: 0.0856 - val_mse: 0.0127\n",
      "Epoch 35/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0709 - mse: 0.0090 - val_loss: 0.0598 - val_mse: 0.0066\n",
      "Epoch 36/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0714 - mse: 0.0094 - val_loss: 0.0753 - val_mse: 0.0093\n",
      "Epoch 37/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0710 - mse: 0.0094 - val_loss: 0.0900 - val_mse: 0.0135\n",
      "Epoch 38/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0671 - mse: 0.0082 - val_loss: 0.0938 - val_mse: 0.0142\n",
      "Epoch 39/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0677 - mse: 0.0083 - val_loss: 0.0623 - val_mse: 0.0067\n",
      "Epoch 40/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0672 - mse: 0.0083 - val_loss: 0.0706 - val_mse: 0.0083\n",
      "Epoch 41/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0653 - mse: 0.0078 - val_loss: 0.0769 - val_mse: 0.0097\n",
      "Epoch 42/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0649 - mse: 0.0077 - val_loss: 0.0650 - val_mse: 0.0068\n",
      "Epoch 43/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0625 - mse: 0.0072 - val_loss: 0.0624 - val_mse: 0.0068\n",
      "Epoch 44/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0632 - mse: 0.0074 - val_loss: 0.0722 - val_mse: 0.0083\n",
      "Epoch 45/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0619 - mse: 0.0071 - val_loss: 0.0607 - val_mse: 0.0060\n",
      "Epoch 46/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0615 - mse: 0.0074 - val_loss: 0.0590 - val_mse: 0.0066\n",
      "Epoch 47/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0605 - mse: 0.0069 - val_loss: 0.0684 - val_mse: 0.0080\n",
      "Epoch 48/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0595 - mse: 0.0066 - val_loss: 0.0900 - val_mse: 0.0136\n",
      "Epoch 49/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0590 - mse: 0.0064 - val_loss: 0.0606 - val_mse: 0.0062\n",
      "Epoch 50/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0578 - mse: 0.0061 - val_loss: 0.0675 - val_mse: 0.0087\n",
      "Epoch 51/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0570 - mse: 0.0064 - val_loss: 0.0675 - val_mse: 0.0077\n",
      "Epoch 52/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0572 - mse: 0.0060 - val_loss: 0.0775 - val_mse: 0.0096\n",
      "Epoch 53/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0555 - mse: 0.0059 - val_loss: 0.0622 - val_mse: 0.0064\n",
      "Epoch 54/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0564 - mse: 0.0059 - val_loss: 0.0654 - val_mse: 0.0088\n",
      "Epoch 55/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0546 - mse: 0.0057 - val_loss: 0.0674 - val_mse: 0.0070\n",
      "Epoch 56/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0533 - mse: 0.0053 - val_loss: 0.0706 - val_mse: 0.0084\n",
      "Epoch 57/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0536 - mse: 0.0054 - val_loss: 0.0774 - val_mse: 0.0101\n",
      "Epoch 58/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0532 - mse: 0.0053 - val_loss: 0.0792 - val_mse: 0.0115\n",
      "Epoch 59/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0532 - mse: 0.0053 - val_loss: 0.0759 - val_mse: 0.0102\n",
      "Epoch 60/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0533 - mse: 0.0054 - val_loss: 0.0662 - val_mse: 0.0070\n",
      "Epoch 61/200\n",
      "699/699 [==============================] - 15s 21ms/step - loss: 0.0518 - mse: 0.0051 - val_loss: 0.0857 - val_mse: 0.0123\n",
      "window_len 45 model Model Evaluation - RMSE: 5.53, MAE: 4.07\n",
      "Epoch 1/200\n",
      "678/678 [==============================] - 20s 23ms/step - loss: 0.6319 - mse: 0.6238 - val_loss: 0.6047 - val_mse: 0.5845\n",
      "Epoch 2/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.5889 - mse: 0.5577 - val_loss: 0.5816 - val_mse: 0.5546\n",
      "Epoch 3/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.5033 - mse: 0.4265 - val_loss: 0.4273 - val_mse: 0.3113\n",
      "Epoch 4/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.3940 - mse: 0.2830 - val_loss: 0.3281 - val_mse: 0.1967\n",
      "Epoch 5/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.3107 - mse: 0.1810 - val_loss: 0.2820 - val_mse: 0.1524\n",
      "Epoch 6/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.2530 - mse: 0.1220 - val_loss: 0.2064 - val_mse: 0.0811\n",
      "Epoch 7/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.2073 - mse: 0.0828 - val_loss: 0.1873 - val_mse: 0.0614\n",
      "Epoch 8/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.1847 - mse: 0.0645 - val_loss: 0.1653 - val_mse: 0.0502\n",
      "Epoch 9/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.1682 - mse: 0.0529 - val_loss: 0.1273 - val_mse: 0.0329\n",
      "Epoch 10/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.1522 - mse: 0.0427 - val_loss: 0.1305 - val_mse: 0.0314\n",
      "Epoch 11/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.1365 - mse: 0.0344 - val_loss: 0.1089 - val_mse: 0.0234\n",
      "Epoch 12/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.1270 - mse: 0.0298 - val_loss: 0.1025 - val_mse: 0.0201\n",
      "Epoch 13/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.1220 - mse: 0.0280 - val_loss: 0.0974 - val_mse: 0.0177\n",
      "Epoch 14/200\n",
      "678/678 [==============================] - 17s 26ms/step - loss: 0.1173 - mse: 0.0248 - val_loss: 0.0943 - val_mse: 0.0173\n",
      "Epoch 15/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.1126 - mse: 0.0233 - val_loss: 0.0981 - val_mse: 0.0177\n",
      "Epoch 16/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.1064 - mse: 0.0208 - val_loss: 0.1033 - val_mse: 0.0179\n",
      "Epoch 17/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.1044 - mse: 0.0198 - val_loss: 0.0931 - val_mse: 0.0148\n",
      "Epoch 18/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.0997 - mse: 0.0180 - val_loss: 0.0805 - val_mse: 0.0120\n",
      "Epoch 19/200\n",
      "678/678 [==============================] - 22s 32ms/step - loss: 0.0965 - mse: 0.0168 - val_loss: 0.0780 - val_mse: 0.0109\n",
      "Epoch 20/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.0902 - mse: 0.0147 - val_loss: 0.0730 - val_mse: 0.0094\n",
      "Epoch 21/200\n",
      "678/678 [==============================] - 4135s 6s/step - loss: 0.0890 - mse: 0.0144 - val_loss: 0.0747 - val_mse: 0.0113\n",
      "Epoch 22/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0862 - mse: 0.0134 - val_loss: 0.0758 - val_mse: 0.0096\n",
      "Epoch 23/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0822 - mse: 0.0123 - val_loss: 0.0841 - val_mse: 0.0119\n",
      "Epoch 24/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0812 - mse: 0.0119 - val_loss: 0.0714 - val_mse: 0.0091\n",
      "Epoch 25/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0823 - mse: 0.0123 - val_loss: 0.0774 - val_mse: 0.0108\n",
      "Epoch 26/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0772 - mse: 0.0107 - val_loss: 0.0656 - val_mse: 0.0074\n",
      "Epoch 27/200\n",
      "678/678 [==============================] - 15s 23ms/step - loss: 0.0759 - mse: 0.0105 - val_loss: 0.0683 - val_mse: 0.0083\n",
      "Epoch 28/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0753 - mse: 0.0103 - val_loss: 0.0850 - val_mse: 0.0121\n",
      "Epoch 29/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0725 - mse: 0.0094 - val_loss: 0.0822 - val_mse: 0.0129\n",
      "Epoch 30/200\n",
      "678/678 [==============================] - 18s 26ms/step - loss: 0.0707 - mse: 0.0093 - val_loss: 0.0783 - val_mse: 0.0099\n",
      "Epoch 31/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0709 - mse: 0.0095 - val_loss: 0.0608 - val_mse: 0.0073\n",
      "Epoch 32/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0679 - mse: 0.0084 - val_loss: 0.0535 - val_mse: 0.0050\n",
      "Epoch 33/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0690 - mse: 0.0088 - val_loss: 0.0843 - val_mse: 0.0115\n",
      "Epoch 34/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0665 - mse: 0.0080 - val_loss: 0.0839 - val_mse: 0.0115\n",
      "Epoch 35/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0656 - mse: 0.0078 - val_loss: 0.0785 - val_mse: 0.0101\n",
      "Epoch 36/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0632 - mse: 0.0075 - val_loss: 0.0723 - val_mse: 0.0089\n",
      "Epoch 37/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0617 - mse: 0.0070 - val_loss: 0.0597 - val_mse: 0.0059\n",
      "Epoch 38/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0623 - mse: 0.0072 - val_loss: 0.0976 - val_mse: 0.0144\n",
      "Epoch 39/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0611 - mse: 0.0069 - val_loss: 0.0897 - val_mse: 0.0137\n",
      "Epoch 40/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0591 - mse: 0.0066 - val_loss: 0.0855 - val_mse: 0.0130\n",
      "Epoch 41/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0593 - mse: 0.0066 - val_loss: 0.0736 - val_mse: 0.0094\n",
      "Epoch 42/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0572 - mse: 0.0060 - val_loss: 0.0528 - val_mse: 0.0046\n",
      "Epoch 43/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0586 - mse: 0.0063 - val_loss: 0.0979 - val_mse: 0.0170\n",
      "Epoch 44/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0562 - mse: 0.0058 - val_loss: 0.0568 - val_mse: 0.0061\n",
      "Epoch 45/200\n",
      "678/678 [==============================] - 15s 23ms/step - loss: 0.0554 - mse: 0.0057 - val_loss: 0.0795 - val_mse: 0.0100\n",
      "Epoch 46/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0549 - mse: 0.0057 - val_loss: 0.0724 - val_mse: 0.0095\n",
      "Epoch 47/200\n",
      "678/678 [==============================] - 15s 23ms/step - loss: 0.0540 - mse: 0.0055 - val_loss: 0.0558 - val_mse: 0.0056\n",
      "Epoch 48/200\n",
      "678/678 [==============================] - 15s 22ms/step - loss: 0.0540 - mse: 0.0056 - val_loss: 0.0703 - val_mse: 0.0086\n",
      "Epoch 49/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0520 - mse: 0.0052 - val_loss: 0.0536 - val_mse: 0.0052\n",
      "Epoch 50/200\n",
      "678/678 [==============================] - 15s 23ms/step - loss: 0.0515 - mse: 0.0050 - val_loss: 0.0652 - val_mse: 0.0072\n",
      "Epoch 51/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0504 - mse: 0.0049 - val_loss: 0.0696 - val_mse: 0.0080\n",
      "Epoch 52/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0506 - mse: 0.0049 - val_loss: 0.0676 - val_mse: 0.0081\n",
      "Epoch 53/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0499 - mse: 0.0048 - val_loss: 0.0816 - val_mse: 0.0113\n",
      "Epoch 54/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0484 - mse: 0.0044 - val_loss: 0.0751 - val_mse: 0.0102\n",
      "Epoch 55/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0480 - mse: 0.0044 - val_loss: 0.0793 - val_mse: 0.0109\n",
      "Epoch 56/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0485 - mse: 0.0045 - val_loss: 0.0434 - val_mse: 0.0036\n",
      "Epoch 57/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0467 - mse: 0.0042 - val_loss: 0.0589 - val_mse: 0.0069\n",
      "Epoch 58/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0460 - mse: 0.0041 - val_loss: 0.0881 - val_mse: 0.0135\n",
      "Epoch 59/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0466 - mse: 0.0043 - val_loss: 0.0633 - val_mse: 0.0067\n",
      "Epoch 60/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0466 - mse: 0.0043 - val_loss: 0.0837 - val_mse: 0.0101\n",
      "Epoch 61/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0453 - mse: 0.0040 - val_loss: 0.0819 - val_mse: 0.0109\n",
      "Epoch 62/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0445 - mse: 0.0039 - val_loss: 0.0775 - val_mse: 0.0101\n",
      "Epoch 63/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0437 - mse: 0.0036 - val_loss: 0.0789 - val_mse: 0.0101\n",
      "Epoch 64/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0428 - mse: 0.0034 - val_loss: 0.0632 - val_mse: 0.0071\n",
      "Epoch 65/200\n",
      "678/678 [==============================] - 17s 26ms/step - loss: 0.0432 - mse: 0.0036 - val_loss: 0.0886 - val_mse: 0.0122\n",
      "Epoch 66/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.0425 - mse: 0.0035 - val_loss: 0.0577 - val_mse: 0.0061\n",
      "Epoch 67/200\n",
      "678/678 [==============================] - 18s 27ms/step - loss: 0.0417 - mse: 0.0034 - val_loss: 0.0597 - val_mse: 0.0074\n",
      "Epoch 68/200\n",
      "678/678 [==============================] - 19s 28ms/step - loss: 0.0423 - mse: 0.0036 - val_loss: 0.0888 - val_mse: 0.0123\n",
      "Epoch 69/200\n",
      "678/678 [==============================] - 19s 28ms/step - loss: 0.0412 - mse: 0.0035 - val_loss: 0.0694 - val_mse: 0.0077\n",
      "Epoch 70/200\n",
      "678/678 [==============================] - 17s 26ms/step - loss: 0.0424 - mse: 0.0035 - val_loss: 0.0717 - val_mse: 0.0086\n",
      "Epoch 71/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0404 - mse: 0.0032 - val_loss: 0.0828 - val_mse: 0.0111\n",
      "Epoch 72/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0401 - mse: 0.0031 - val_loss: 0.0556 - val_mse: 0.0056\n",
      "Epoch 73/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0397 - mse: 0.0031 - val_loss: 0.0627 - val_mse: 0.0074\n",
      "Epoch 74/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0387 - mse: 0.0030 - val_loss: 0.0585 - val_mse: 0.0070\n",
      "Epoch 75/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0389 - mse: 0.0029 - val_loss: 0.0638 - val_mse: 0.0074\n",
      "Epoch 76/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0389 - mse: 0.0029 - val_loss: 0.0876 - val_mse: 0.0122\n",
      "Epoch 77/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0386 - mse: 0.0030 - val_loss: 0.0780 - val_mse: 0.0108\n",
      "Epoch 78/200\n",
      "678/678 [==============================] - 15s 23ms/step - loss: 0.0383 - mse: 0.0030 - val_loss: 0.0651 - val_mse: 0.0073\n",
      "Epoch 79/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0384 - mse: 0.0029 - val_loss: 0.0813 - val_mse: 0.0104\n",
      "Epoch 80/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0374 - mse: 0.0028 - val_loss: 0.0671 - val_mse: 0.0081\n",
      "Epoch 81/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0374 - mse: 0.0030 - val_loss: 0.0457 - val_mse: 0.0044\n",
      "Epoch 82/200\n",
      "678/678 [==============================] - 17s 26ms/step - loss: 0.0379 - mse: 0.0032 - val_loss: 0.0735 - val_mse: 0.0097\n",
      "Epoch 83/200\n",
      "678/678 [==============================] - 16s 24ms/step - loss: 0.0366 - mse: 0.0027 - val_loss: 0.0850 - val_mse: 0.0111\n",
      "Epoch 84/200\n",
      "678/678 [==============================] - 16s 23ms/step - loss: 0.0366 - mse: 0.0027 - val_loss: 0.0673 - val_mse: 0.0072\n",
      "Epoch 85/200\n",
      "678/678 [==============================] - 15s 23ms/step - loss: 0.0362 - mse: 0.0027 - val_loss: 0.0736 - val_mse: 0.0102\n",
      "Epoch 86/200\n",
      "678/678 [==============================] - 17s 25ms/step - loss: 0.0355 - mse: 0.0025 - val_loss: 0.0790 - val_mse: 0.0108\n",
      "window_len 50 model Model Evaluation - RMSE: 4.16, MAE: 3.11\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(window_size)):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size[i], test_size=0.2)\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True, input_shape=(window_size[i], num_features)),\n",
    "        Dropout(0.3),\n",
    "        GRU(32, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(16, return_sequences=False),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),loss='mae',metrics=['mse'])\n",
    "    # Train the model\n",
    "    history = model.fit(X_train,y_train,\n",
    "                        validation_data=(X_val,y_val),\n",
    "                        epochs=200, batch_size=16,\n",
    "                        callbacks=tf.keras.callbacks.EarlyStopping(patience=30,monitor='val_loss',restore_best_weights=True))\n",
    "    historys.append(history)\n",
    "    model.save(f'models/model_winLen{window_size[i]}.keras')\n",
    "    # X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size, test_size=0.2)\n",
    "    metrics_results.append(evaluate_model(model, X_test,y_test,f'window_len {window_size[i]} model',yscaler))\n",
    "    del model, X_train, X_val, X_test, y_train, y_val, y_test, history\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_vs_window(results, window_lengths):\n",
    "    \"\"\"\n",
    "    Plots MAE and RMSE vs. window lengths.\n",
    "    \n",
    "    Parameters:\n",
    "        results (list of tuples): Each tuple contains (rmse, mae) for a model.\n",
    "        window_lengths (list): List of window lengths corresponding to each result.\n",
    "    \"\"\"\n",
    "    # Extract RMSE and MAE from results\n",
    "    rmse_values = [result[1] for result in results]\n",
    "    mae_values = [result[2] for result in results]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(window_lengths, rmse_values, label=\"RMSE\", marker='o', color='b')\n",
    "    plt.plot(window_lengths, mae_values, label=\"MAE\", marker='o', color='r')\n",
    "    \n",
    "    plt.title(\"Model Performance vs. Window Length\")\n",
    "    plt.xlabel(\"Window Length\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics_vs_window(metrics_results, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming 'history_array' is a list of history objects\n",
    "with open('history_array.pkl', 'wb') as f:\n",
    "    pickle.dump([h.history for h in historys], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history_array.pkl', 'rb') as f:\n",
    "    history_arrays = pickle.load(f)\n",
    "\n",
    "# 'history_arrays' will now be a list of dictionaries, each containing the training history for each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
